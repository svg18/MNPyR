# Validación de supuestos

Como vimos anteriormente, debemos validar los supuestos del modelo de regresión lineal múltiple:

* Supuesto de normalidad

* Supuesto de linealidad

* Supuesto de homocedasticidad

* Valores outlier e influyentes

* Supuesto de multicolinealidad

Los primeros enlistados ya se vieron en la sección anterior, lo que vamos a introducir ahora es el supuesto de multicolinealidad.

## Supuesto de multicolinealidad

Se dice que el ajuste del modelo lineal sobre una muestra tiene problemas de multicolinealidad cuando hay una correlación alta entre dos o más variables explicativas, consecuencia de que una variable regresora es linealmente dependiente a alguna o algunas de las demás variables de la matriz diseño $X$.

Un modelo de regresión lineal con ésta característica provoca un error en la estimación de los parámetros $\underline{\beta}$, ya que al tener variables linealmente dependientes el determinante $\mid X'X \mid \ \rightarrow 0$, lo cual provoca que la inversa $(X'X)^{-1} \rightarrow \infty$.
Esto provoca el cálculo erróneo de los parámetros, ya que el producto matricial de $\underline{\hat{\beta}}=(X'X)^{-1}X'\underline{Y}$ no puede ser aproximado de la mejor manera.


## Detección de multicolinealidad

Debido al problema de estimación de los parámetros $\underline{\beta}$ que ocasiona un modelo con multicolinealidad, se realizan validaciones con la finalidad de tener evidencia suficiente para asumir si el modelo posee o no el problema de dependencia lineal de las variables explicativas.

El método más adecuado para efectuar ésta validación, es encontrar en la matriz de diseño $X$ variables regresoras linealmente dependendientes a través de operaciones matriciales elementales, sin embargo, este procedimiento es tardado conforme el tamaño de la muestra es mayor, además de que computacionalmente el procedimiento no es óptimo, es por ello que se desarrollan diferentes métodos para conocer si el modelo presenta multicolinealidad.

**A través de correlaciones**

El primer método para detectar la dependencia lineal es analizar el coeficiente de correlación de las variables, ya que si el modelo posee variables altamente correlacionadas entre sí, es probable que el ajuste lineal presente problemas de multicolinealidad debido a la relación estrecha de éstas variables, una desventaja de éste método es que analiza a través de la matriz de correlaciones de una variable comparada con otra, sin embargo, puede existir una baja correlación entre variables analizándolas una a una pero al analizarlas de dos en dos, o de tres en tres, etcétera, puede encontrarse una alta correlación.

Para analizar de mejor forma la información, se obtiene el determinante de la matriz de correlaciones, si este es muy cercano a cero podría proporcionar indicios de problemas de multicolinealidad, debido a que si hay dos variables que son linealmente dependientes el determinante de la matriz de correlaciones tenderá a cero. Sin embargo, depende del prejuicio del investigador a partir de qué cifra debe ser considerada una cantidad mínima.

**A través del índice $\kappa$**

En 1960, Jacob Cohen, presentó el método del coeficiente kappa. El coeficiente kappa o simplemente $\kappa$ se basa en el análisis de los eigenvalores $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ de la matriz $(X'X),$ este procedimiento es llamado $análisis \ del \ egeinsistema.$

Debido a que $(X'X)$ es una matriz simétrica, el producto de la matriz diseño es igual a sus valores propios o eigenvalores. De esta manera el método propone observar la proporción del eigenvalor más grande respecto al más pequeño, si la proporción es pequeña, no hay problemas de multicolinealidad pues todos los valores propios son similares, sin embargo, proporciones altas indican gran variabilidad en los valores propios por lo que se tiene indicios de multicolinealidad.
De esta manera el coeficiente $\kappa$ es calculado como:

$$\kappa=\frac{\lambda_{max}}{\lambda_{min}}.$$

## Ejemplo 

Se tiene información sobre las cajetillas vendidas y queremos saber que factores influyen en dichas ventas.

```{r}
cigarros=read.table("datos cigarros.csv",sep=",",header=TRUE)
attach(cigarros)
```

Lo primero que hay que hacer es identificar las variables que hay en la base y realizar diagramas de dispersión para tener una idea de que tipo de relación existe entre las variables.

```{r,echo=FALSE}
head(cigarros)
```

$\textbf{y}=\textbf{SCIG} \ : \ \mbox{Cajetillas de cigarro vendidas}$

$\textbf{x}_1=\textbf{AGE} \ : \ \mbox{Edad promedio}$

$\textbf{x}_2=\textbf{ED} \ : \ \mbox{Porcentaje de personas con más de 25 años}$

$\textbf{x}_3=\textbf{PERFEM} \ : \ \mbox{Porcentaje de mujeres}$

$\textbf{x}_4=\textbf{PRICE} \ : \ \mbox{Precio}$


```{r,echo=FALSE}
summary(cigarros)
```


**Ajuste de un modelo de RLM**

Ajustaremos un modelo de RLM considerando lo siguiente:

* Emplearemos el método $\textbf{backward}.$

* El criterio de selección del $mejor \ modelo$ estará basado en el $\textbf{AIC}$

El método **backward** (eliminación hacia atrás) consiste en comenzar con un modelo que incluya las $k$ variables regresoras y luego ir eliminando varible por variable de acuerdo a cierto criterio.

El **AIC** (Criterio de información de Akaike) es una medida de la calidad de un modelo estadístico, considera la $bondad \ de \ ajuste$ del modelo y su complejidad.

$$AIC=-2log(L) +2* n_{parámetros}$$
El mejor modelo es aquel que tenga un $\textbf{AIC}$ pequeño.

**Primer Paso: Relación entre Variables**

Primero haremos las gráficas de dispersión correspondientes para ver el comportamiento de los datos.

Diagramas de dispersion por parejas: Primero juntamos todas las variables explicativas y la dependiente en un solo objeto para hacer los diagramas de dispersion para cada pareja

```{r}

cigarrosvars=data.frame(cigarros$SCIG,cigarros$AGE,cigarros$ED,cigarros$PERFEM,cigarros$PRICE)
cor(cigarrosvars)
pairs(cigarrosvars,main="Matriz de dispersion",labels=c("Cajetillas vendidas", "Edad promedio", "Porcentaje con mas de 25", "Porcentaje mujeres", "Precio promedio"),pch=21,bg="deeppink4")
```

A simple vista "no estamos apreciando toda la información", haremos gráficos dos a dos:

```{r}
plot(cigarros$AGE,cigarros$SCIG,type = "p",col="deeppink4",pch=16,
xlab="Edad promedio", ylab="Cajetillas vendidas",
main= "Relación entre la edad promedio y las cajetillas vendidas")
```


```{r}
plot(cigarros$ED,cigarros$SCIG,type = "p",col="deeppink4",pch=16,
xlab="Porcentaje con más de 25", ylab="Cajetillas vendidas",
main= "Relación entre el porcentaje con más de 25 y las cajetillas vendidas")
```

```{r}
plot(cigarros$PERFEM,cigarros$SCIG,type = "p",col="deeppink4",pch=16,
xlab="Porcentaje de mujeres", ylab="Cajetillas vendidas",
main= "Relación entre el porcentaje de mujeres y las cajetillas vendidas")
```

```{r}
plot(cigarros$PRICE,cigarros$SCIG,type = "p",col="deeppink4",pch=16,
xlab="Precio", ylab="Cajetillas vendidas",
main= "Relación entre el precio y las cajetillas vendidas")
```

Observemos que tanto la variable de promedio de más de 25 y las cajetillas vendidas  muestran una relación, al igual que la variable precio.

Veamos ahora la correlación entre las variables:

```{r}
cor(cigarros)
```

Las correlaciones entre las variables que no son las cajetillas vendidas no son "altas" por lo que en principio no parece haber problema de considerar todas las variables dentro del modelo.

**Segundo Paso: Generación de Modelos y Elección del Mejor Modelo**

Emplearemos comandos de R:

Empezaremos con el modelo inicial, le llamamos así ya que es el modelo más grande que podemos suponer, ya que el modelo explica las cajetillas vendidas en función de las otras 4 variables:

```{r}
modeloini=lm(SCIG~AGE+ED+PERFEM+PRICE, data=cigarros) 
summary(modeloini)
```
De este primer modelo dado que las pruebas de hipótesis para cada $\beta_{i}$ nos dicen que la única variable significativa es $\beta_{4}$ que es para la variable Precio. Y hasta el momento tiene sentido esto.

Ahora si, emplearemos el método backward con el criterio AIC.

```{r}
modelo_backward_AIC=stepAIC(modeloini,direction = "backward")
```

Ahora observemos el mejor modelo, de acuerdo a lo arrojado en **stepAIC()**.

```{r}
summary(modelo_backward_AIC)
```

A través de la tablas ANOVA se tiene:

```{r}
anova(modelo_backward_AIC)
```

De los resultados obtenidos podemos refinar el $mejor \ modelo$ obtenido:

```{r}
modelo_refinado=lm(SCIG~0+AGE+PRICE,cigarros)
summary(modelo_refinado)
```
Entonces ya obtuvimos el mejor modelo, que está compuesto por:

$$\mbox{Cajetillas vendidas}= 7.809*\mbox{Edad promedio}-2.477*\mbox{Precio}$$
Bueno todavía debemos validar los supuestos:

**Tercer Paso Validadción de Supuestos**

Siguiendo el orden de nuestros capítulos, para validar gráficamente la normalidad de los errores debemos graficar los errores contra los cuantiles de la distribución normal.Para ello aplicamos la función **qqnorm** y con **qqline** obtenemos la recta diagonal que nos servirá para ver que tan lejos o cerca de la distribución normal están cayendo los residuales.

```{r}
qqnorm(rstandard(modelo_refinado),ylim = c(-2,2),xlim = c(-2,2))
qqline(rstandard(modelo_refinado),distribution = qnorm,col="deeppink4")
```

Podemos observar que la parte central de la distribución si se ajusta a una distribución normal, sin embargo, en los extremos los residuales ya no se comportan como una distribución normal.

Podemos aplicar la prueba de bondad de ajuste **Lilliefors para normalidad** vista en Bondad de Ajuste:

```{r}
nortest::lillie.test(rstandard(modelo_refinado))
```

Como el valor del $p-value$ es menor al nivel de significancia $\alpha=0.05$ entonces  rechazamos $H_{0}$, es decir nuestros residuales  no tienen distribución normal.

**Supuesto de Linealidad**

Como se menciona en el capítulo, graficaremos los errores estandarizados contra los valores observados de la variable explicativa.

```{r,echo=FALSE}
plot(cigarros$AGE,rstandard(modelo_refinado),ylim = c(-3,3),col="deeppink4",pch=16,xlab="cajetillas vendidas", ylab="Errores estandarizados" ) 

abline(a=0,b=0,col="blue")

```

```{r,echo=FALSE}
plot(cigarros$PRICE,rstandard(modelo_refinado),ylim = c(-3,3),col="deeppink4",pch=16,xlab="cajetillas vendidas", ylab="Errores estandarizados" ) 

abline(a=0,b=0,col="blue")

```

Para ambas variables, salvo por la posible presencia de algunos valores atípicos que salen de la franja horizontal $(-2,2)$, el resto de las observaciones parecen distribuirse como ruido blanco.

**Supuesto de Homocedasticidad**

Se dice que una muestra es homocedástica cuando la varianza es constante a lo largo de todas las observaciones, es decir, no varia conforme se presentan nuevas observaciones.

```{r,echo=FALSE}
plot(modelo_refinado$fitted.values,rstandard(modelo_refinado),ylim = c(-3,3),col="deeppink4",pch=16,xlab="Valores Ajustados", ylab="Errores estandarizados" ) 

abline(a=0,b=0,col="blue")

abline(a=-2,b=0,col="green4")

abline(a=2,b=0,col="green4")
```


* Si la varianza es constante entonces la gráfica fluctuaráentre el eje horizontal de manera simétrica, y sin seguir algún patrón, y se espera que la mayor parte de los errores estén contenidos en franjas horizontales delimitados por el eje entre -2 y 2. En éste ejemplo la dispersión regular de los residuales dentro de las Bandas superior e inferior y que no haya residuales que se alejen tanto de la Banda 0, indican varianza constante. 

Adicionalmente aplicaremos las pruebas vistas en el capítulo para tener certeza estadística de la validez del supuesto de homocedasticidad.

**Prueba White**

```{r}
dataset=data.frame(cigarros$AGE,cigarros$PRICE,cigarros$SCIG)
mode1=VAR(dataset,p=1)
#whites.htest(mode1)
```

Por el $p-value$, la hipótesis de homocedasticidad no se rechaza.

**Supuesto de Multicolinealidad**

```{r}
X1=scale(cigarros[,-5])

A=t(X1)%*%X1

kappa=max(eigen(A)$values)/min(eigen(A)$values)
kappa
```
El coeficiente $kappa$ es de 3.15 por lo tanto no tenemos problemas de multicolinealidad.






# Apéndice

Será conveniente desarrollar algunas variantes en la forma en la que se denota a los residuales, para ello se define a la matriz $H$ como $H=X(X'X)^{-1}X'$,es conocida como **"matriz sombrero"**, que junto con la matriz $(I-H)$ cumplen con ser matrices idempotentes, es decir, que al elevar las matrices a una potencia dada los valores contenidos en la matriz no se modifican; de igual forma ambas matrices cumplen con ser simétricas, denominadas así ya que al transponer las matrices los valores contenidos en ellas conservan su lugar.

Debemos considerar el siguiente resultado, el cual será importante al desarrollar el siguiente teorema A ya que demuestra que $(X'X)^{-1}$ es una matriz simétrica.

$$[(X'X)^{-1}]'=[(X'X)']^{-1}$$
$$=(X'(X')')^{-1}$$
$$\therefore [(X'X)^{-1}]'= (X'X)^{-1}. \blacksquare$$
Es decir, la inversa de $X'X$ es simétrica, resultado importante en el siguiente teorema:

**Teorema A** Sea $H=X(X'X)^{-1}X'$ e $(I-H)$ entonces:

**a)** Las matrices $H$ e $I-H$ son idempotentes.

**b)** Las matrices $H$ e $I-H$ son simétricas.

**Demostración:**

**a)** Para demostrar la idempotencia de $H$ basta probar que $H^2=H,$ es decir, al elevar la matriz $H$ ésta no se alterará:

$$H^2=(X(X'X)^{-1}X')(X(X'X)^{-1}X')$$
$$=X(X'X)^{-1}X'X(X'X)^{-1}X'.$$
Tranponiendo con la finalidad de simplificar el producto matricial y por el resultado mostrado anteriormente $[(X'X)^{-1}]'=(X'X)^{-1}$ se tiene:

$$=[(X'X)^{-1}X'X(X'X)^{-1}X']'X'$$
$$=[(X'X)^{-1}X']'X'$$
$$=X(X'X)^{-1}X'$$

$$\therefore H^2=H.$$
Por lo tanto $H$ es idempotente. $\blacksquare$

Para probar la idempotencia de $I-H$, ésta será elevada al cuadrado.

$$(I-H)^2=(I-H)(I-H)$$
$$=I-IH-IH+H^2$$
$$=I-2H+H^2.$$

Por idempotencia de $H$, $H=H^2$. Por lo tanto:
$$(I-H)=I-2H+H$$
$$\therefore (I-H)^2=I-H.$$

**b)** Para demostrar la simetría de $H$, se transpondrá la matriz $H$. Además debemos recordar que $[(X'X)^{-1}]'=(X'X)^{-1}$ así:

$$H'= (X(X'X)^{-1}X')'$$
$$=X(X'X)^{-1}X'$$
$$\therefore H'= H.$$

Por lo tanto la matriz $H$ es simétrica.

Para la simetría de $I-H$ se transpone la matriz:

$$(I-H)'=I'-H'.$$
Por simetría de H y de I

$$\therefore (I-H)^2=I-H$$

Por lo tanto $I-H$ es simétrica. $\blacksquare$

**Corolario A** Sea $\underline{e}$ la matriz de residuales entonces estos pueden ser expresados por la siguiente ecuación:

$$\underline{e}=(I-H)\underline{Y}$$
donde $I$ es la matriz identidad, y $H=X(X'X)^{-1}X'.$

**Demostración:**

Se sabe que los valores estimados son calculados de la siguiente manera:

$$\underline{\hat{Y}}=X\underline{\hat{\beta}}$$
$$\underline{\hat{Y}}=X(X'X)^{-1}X'\underline{Y}$$
$$\underline{\hat{Y}}=H\underline{Y}.$$
donde $H=X(X'X)^{-1}X'.$ De esta manera calculando la matriz de residuales se tiene:

$$\underline{e}=\underline{Y}-\underline{\hat{Y}}$$
$$\underline{e}=\underline{Y}-X\underline{\hat{\beta}}$$
$$\underline{e}=\underline{Y}-H\underline{Y}$$
$$\underline{e}=(I-H)\underline{Y}.\blacksquare$$

**Teorema B** Sea una variable de interés $\underline{Y}$, llamada **dependiente**, relacionada con dos o más variables explicativas$x_{1},x_{2},\ldots,x_{k}$, 
entonces:

**a)** $\mathbf{E}[\underline{Y}]= \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots + \beta_{k}x_{k}.$ 

**b)** $Var(\underline{Y})= \sigma^2.$

**Demostración:**

**a)** Para la esperanza de $\underline{Y}$ se tiene:

$$\mathbf{E}[\underline{Y}]=\mathbf{E}[\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots +\beta_{k}x_{k}+\epsilon].$$
La estimación es sobre $\underline{Y},$
como $\beta_{0},\beta_{1},\beta_{2},\ldots,\beta_{k}$ son constantes; $x_{1},x_{2}, \ldots,x_{k}$ son los valores dados, por lo que:

$$\mathbf{E}[\underline{Y}]=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots +\beta_{k}x_{k}+\mathbf{E}[\epsilon].$$

$$=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots +\beta_{k}x_{k}+0$$
$$\therefore \mathbf{E}[\underline{Y}]= \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots + \beta_{k}x_{k}. \blacksquare$$ 

**b)** Para la varianza de $\underline{Y}$ se tiene:

$$Var(\underline{Y})=Var\left( \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+ \ldots + \beta_{k}x_{k}+ \epsilon\right).$$
La estimación es sobre $\underline{Y}$, $\beta_{0},\beta_{1},\beta_{2},\ldots,\beta_{k}$ sons constantes;$x_{1},x_{2},\ldots,x_{k}$ son valores dados, por lo que cumple que:

$$Var(\underline{Y})=0+0+0+\ldots+0+Var(\epsilon)$$
$$\therefore Var(\underline{Y})=\sigma^2.\blacksquare$$


