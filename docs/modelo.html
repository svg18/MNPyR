<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Modelo | Modelos de Regresión</title>
  <meta name="description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Modelo | Modelos de Regresión" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Modelo | Modelos de Regresión" />
  
  <meta name="twitter:description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />
<meta name="author" content="Dulce María Reyes Varela" />
<meta name="author" content="Carlos Fernando Vásquez Guerra" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducción-1.html"/>
<link rel="next" href="intervalos-de-confianza-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelos no paramétricos y de Regresión</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i>Objetivos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I Introduccion</b></span></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i>Modelos lineales</a>
<ul>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#principios-de-la-modelización-estadística"><i class="fa fa-check"></i>Principios de la modelización estadística</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#identificar-y-caracterizar-variables"><i class="fa fa-check"></i>Identificar y Caracterizar Variables</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#tipos-de-variables-y-tipo-de-modelo"><i class="fa fa-check"></i>Tipos de variables y tipo de modelo</a>
<ul>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-las-variables-explicativas"><i class="fa fa-check"></i>En función de las variables explicativas</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-la-variable-respuesta"><i class="fa fa-check"></i>En función de la variable respuesta</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-modelo-lineal-general"><i class="fa fa-check"></i>El modelo lineal general</a></li>
</ul></li>
<li class="part"><span><b>II Regresión Lineal Simple</b></span></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a>
<ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i>Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i>El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#regresión-lineal-simple"><i class="fa fa-check"></i>Regresión lineal simple</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html"><i class="fa fa-check"></i><b>1</b> Modelo con intercepto</a>
<ul>
<li class="chapter" data-level="1.1" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo"><i class="fa fa-check"></i><b>1.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="1.2" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html#propiedades-de-los-estimadores"><i class="fa fa-check"></i><b>1.2</b> Propiedades de los estimadores</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html"><i class="fa fa-check"></i><b>2</b> Modelo sin intercepto</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-1"><i class="fa fa-check"></i><b>2.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.2" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#propiedades-de-los-estimadores-1"><i class="fa fa-check"></i><b>2.2</b> Propiedades de los estimadores</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#ejemplo-en-r-studio"><i class="fa fa-check"></i><b>2.2.1</b> Ejemplo en R-Studio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html"><i class="fa fa-check"></i><b>3</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="3.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-beta_0"><i class="fa fa-check"></i><b>3.1</b> Intervalo para <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-beta_1"><i class="fa fa-check"></i><b>3.2</b> Intervalo para <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-sigma2"><i class="fa fa-check"></i><b>3.3</b> Intervalo para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-el-valor-esperado-y"><i class="fa fa-check"></i><b>3.4</b> Intervalo para el valor esperado <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-de-predicción"><i class="fa fa-check"></i><b>3.5</b> Intervalo de predicción</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#ejemplo"><i class="fa fa-check"></i><b>3.5.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html"><i class="fa fa-check"></i><b>4</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-para-beta_0"><i class="fa fa-check"></i><b>4.1</b> Pruebas para <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="4.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#prueba-para-beta_1"><i class="fa fa-check"></i><b>4.2</b> Prueba para <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#prueba-para-sigma2"><i class="fa fa-check"></i><b>4.3</b> Prueba para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#análisis-de-la-varianza-anova"><i class="fa fa-check"></i><b>4.4</b> Análisis de la varianza (ANOVA)</a></li>
<li class="chapter" data-level="4.5" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#coeficiente-de-determinación"><i class="fa fa-check"></i><b>4.5</b> Coeficiente de determinación</a></li>
<li class="chapter" data-level="4.6" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#propiedades-de-r2"><i class="fa fa-check"></i><b>4.6</b> Propiedades de <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="4.7" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#relación-r2-y-la-correlación-de-pearson"><i class="fa fa-check"></i><b>4.7</b> Relación <span class="math inline">\(R^2\)</span> y la correlación de Pearson</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#ejemplo-1"><i class="fa fa-check"></i><b>4.7.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html"><i class="fa fa-check"></i><b>5</b> Validación de supuestos</a>
<ul>
<li class="chapter" data-level="5.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#análisis-de-residuales"><i class="fa fa-check"></i><b>5.1</b> Análisis de residuales</a></li>
<li class="chapter" data-level="5.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-normalidad"><i class="fa fa-check"></i><b>5.2</b> Supuesto de normalidad</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#validación-del-supuesto-de-normalidad"><i class="fa fa-check"></i><b>5.2.1</b> Validación del supuesto de normalidad</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-linealidad"><i class="fa fa-check"></i><b>5.3</b> Supuesto de linealidad</a></li>
<li class="chapter" data-level="5.4" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-homocedasticidad"><i class="fa fa-check"></i><b>5.4</b> Supuesto de homocedasticidad</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#prueba-de-breusch-pagan"><i class="fa fa-check"></i><b>5.4.1</b> Prueba de Breusch-Pagan</a></li>
<li class="chapter" data-level="5.4.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#prueba-de-white"><i class="fa fa-check"></i><b>5.4.2</b> Prueba de White</a></li>
<li class="chapter" data-level="5.4.3" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#ejemplo-2"><i class="fa fa-check"></i><b>5.4.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-outlier-e-influyentes"><i class="fa fa-check"></i><b>5.5</b> Valores outlier e influyentes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-outlier"><i class="fa fa-check"></i><b>5.5.1</b> Valores outlier</a></li>
<li class="chapter" data-level="5.5.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-influentes"><i class="fa fa-check"></i><b>5.5.2</b> Valores influentes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html"><i class="fa fa-check"></i><b>6</b> Ejemplo de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#contexto-del-problema"><i class="fa fa-check"></i><b>6.1</b> Contexto del problema</a></li>
<li class="chapter" data-level="6.2" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#exploración-de-datos"><i class="fa fa-check"></i><b>6.2</b> Exploración de datos</a></li>
<li class="chapter" data-level="6.3" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#visualización-de-datos"><i class="fa fa-check"></i><b>6.3</b> Visualización de datos</a></li>
<li class="chapter" data-level="6.4" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#ajuste-del-modelo-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>6.4</b> Ajuste del modelo de regresión lineal simple</a></li>
<li class="chapter" data-level="6.5" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#validación-del-modelo"><i class="fa fa-check"></i><b>6.5</b> Validación del modelo</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#coefficients"><i class="fa fa-check"></i><b>6.5.1</b> Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>6.6</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="6.7" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#intepretación-de-resultados"><i class="fa fa-check"></i><b>6.7</b> Intepretación de resultados</a></li>
</ul></li>
<li class="part"><span><b>III Modelo de regresión lineal múltiple</b></span></li>
<li class="chapter" data-level="" data-path="introducción-1.html"><a href="introducción-1.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="7" data-path="modelo.html"><a href="modelo.html"><i class="fa fa-check"></i><b>7</b> Modelo</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modelo.html"><a href="modelo.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-2"><i class="fa fa-check"></i><b>7.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="7.2" data-path="modelo.html"><a href="modelo.html#estimación-por-máxima-verosimilitud"><i class="fa fa-check"></i><b>7.2</b> Estimación por máxima verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html"><i class="fa fa-check"></i><b>8</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="8.1" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalo-para-beta_j"><i class="fa fa-check"></i><b>8.1</b> Intervalo para <span class="math inline">\(\beta_{j}\)</span></a></li>
<li class="chapter" data-level="8.2" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalo-para-sigma2-1"><i class="fa fa-check"></i><b>8.2</b> Intervalo para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="8.3" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalos-de-la-respuesta-media"><i class="fa fa-check"></i><b>8.3</b> Intervalos de la respuesta media</a></li>
<li class="chapter" data-level="8.4" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalos-de-predicción"><i class="fa fa-check"></i><b>8.4</b> Intervalos de predicción</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html"><i class="fa fa-check"></i><b>9</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#región-de-rechazo-para-beta_j"><i class="fa fa-check"></i><b>9.1</b> Región de rechazo para <span class="math inline">\(\beta_{j}\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#prueba-para-sigma2-1"><i class="fa fa-check"></i><b>9.2</b> Prueba para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#análisis-de-la-varianza-anova-1"><i class="fa fa-check"></i><b>9.3</b> Análisis de la varianza (ANOVA)</a></li>
<li class="chapter" data-level="9.4" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#coeficiente-de-determinación-2"><i class="fa fa-check"></i><b>9.4</b> Coeficiente de determinación</a></li>
<li class="chapter" data-level="9.5" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#r2-ajustado"><i class="fa fa-check"></i><b>9.5</b> <span class="math inline">\(R^2\)</span> ajustado</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html"><i class="fa fa-check"></i><b>10</b> Validación de supuestos</a>
<ul>
<li class="chapter" data-level="10.1" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#supuesto-de-multicolinealidad"><i class="fa fa-check"></i><b>10.1</b> Supuesto de multicolinealidad</a></li>
<li class="chapter" data-level="10.2" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#detección-de-multicolinealidad"><i class="fa fa-check"></i><b>10.2</b> Detección de multicolinealidad</a></li>
<li class="chapter" data-level="10.3" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#ejemplo-3"><i class="fa fa-check"></i><b>10.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="apéndice.html"><a href="apéndice.html"><i class="fa fa-check"></i><b>11</b> Apéndice</a></li>
<li class="chapter" data-level="12" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html"><i class="fa fa-check"></i><b>12</b> Ejemplo de regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#contexto-del-problema-1"><i class="fa fa-check"></i><b>12.1</b> Contexto del problema</a></li>
<li class="chapter" data-level="12.2" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#exploración-de-datos-1"><i class="fa fa-check"></i><b>12.2</b> Exploración de datos</a></li>
<li class="chapter" data-level="12.3" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#visualización-de-datos-1"><i class="fa fa-check"></i><b>12.3</b> Visualización de datos</a></li>
<li class="chapter" data-level="12.4" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#ajuste-del-modelo-de-regresión-lineal-múltiple"><i class="fa fa-check"></i><b>12.4</b> Ajuste del modelo de regresión lineal múltiple</a></li>
<li class="chapter" data-level="12.5" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#validación-del-modelo-1"><i class="fa fa-check"></i><b>12.5</b> Validación del modelo</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#coefficients-1"><i class="fa fa-check"></i><b>12.5.1</b> Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#bondad-de-ajuste-1"><i class="fa fa-check"></i><b>12.6</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="12.7" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#intepretación-de-resultados-1"><i class="fa fa-check"></i><b>12.7</b> Intepretación de resultados</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Hecho con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelos de Regresión</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelo" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Capítulo 7</span> Modelo<a href="modelo.html#modelo" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>El objetivo del modelo de regresión lineal múltiple consiste en modelar <span class="math inline">\(\underline{Y}\)</span> a través de <span class="math inline">\(k\)</span> variables regresoras en <span class="math inline">\(n\)</span> observaciones independientes. Es decir, se tiene el siguiente modelo:</p>
<p><span class="math display">\[
\begin{array}{ c c c c c c c c c c c c c }
Y_{1}  &amp; = &amp; \beta_{0} &amp; + &amp; \beta_{1}x_{11}&amp; +&amp; \beta_{2}x_{12} &amp; +&amp; \cdots &amp; + &amp; \beta_{k}x_{1k} &amp; + &amp; \epsilon_{1}  \\
Y_{2}  &amp; = &amp; \beta_{0} &amp; + &amp; \beta_{1}x_{21}&amp; +&amp; \beta_{2}x_{22} &amp; +&amp; \cdots &amp; + &amp; \beta_{k}x_{2k} &amp; + &amp; \epsilon_{2}  \\
\vdots  &amp;  &amp; \vdots &amp;  &amp; \vdots&amp; &amp; \vdots &amp; &amp; \vdots &amp; &amp; \vdots &amp;  &amp; \vdots  \\
Y_{n}  &amp; = &amp; \beta_{0} &amp; + &amp; \beta_{1}x_{n1}&amp; +&amp; \beta_{2}x_{n2} &amp; +&amp; \cdots &amp; + &amp; \beta_{k}x_{nk} &amp; + &amp; \epsilon_{n}  \\
\end{array}
\]</span></p>
<p>El anterior conjunto de igualdades puede ser denotado matricialmente mediante la siguiente igualdad:</p>
<p><span class="math display">\[\underline{Y}=X\underline{\beta}+\underline{\epsilon}\]</span></p>
<p>donde:
<span class="math display">\[
\underline{Y}=
\left(
\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}\\
\end{array}
\right),\ \ \ \  X=
\left(
\begin{array}{c c c c c}
1      &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1k}\\
1      &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1      &amp; x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{nk}\\
\end{array}
\right)
\]</span>
<span class="math display">\[
\underline{\beta}=
\left(
\begin{array}{c}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{k}\\
\end{array}
\right),\ \ \ \  \epsilon=
\left(
\begin{array}{c}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{n}\\
\end{array}
\right)
\]</span>
Sustituyendo en la ecuación anterior, se observa que el modelo de regresión multiple puede ser visto como:
<span class="math display">\[
\left(
\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}\\
\end{array}
\right)=
\left(
\begin{array}{c c c c c}
1      &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1k}\\
1      &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1      &amp; x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{nk}\\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{k}\\
\end{array}
\right) +
\left(
\begin{array}{c}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{n}\\
\end{array}
\right)
\]</span></p>
<p>La dimensión de las matrices señaladas anteriormente se mencionan en el siguiente recuadro:</p>
<p><span class="math display">\[
\begin{array}{|c|c|}
\hline
Matriz &amp; Dimensión \\
\hline
\underline{Y}        &amp; n \times 1 \\
X                   &amp; n \times (k+1) \\
\underline{\epsilon} &amp; n \times 1 \\
\underline{\beta}    &amp; (k+1) \times 1 \\
\hline
\end{array}
\]</span></p>
<p>Finalmente para definir correctamente el modelo es necesario realizar las siguientes suposiciones acerca de las matrices del modelo de regresión lineal múltiple.</p>
<p><strong>Definición 3.2</strong> Sea <span class="math inline">\(X\)</span> la denominada <span class="math inline">\(matriz \ diseño\)</span> entonces satisface que:</p>
<ul>
<li><span class="math inline">\(X_{n \times (k+1)}\)</span> es el rango completo en la columna, es decir, <span class="math inline">\(X\)</span> es de rango <span class="math inline">\(k+1\)</span></li>
</ul>
<p>Éste supuesto es importante ya que satisface que <span class="math inline">\(k+1\leq n\)</span>, es decir, <strong>el máximo número de variables con el que se ajusta el modelo no puede ser superior al número de observaciones.</strong></p>
<p>De igual forma, observe que el supuesto de la varianza de los errores en la <strong>definición 3.1</strong>, puede reescribirse en forma matricial:</p>
<p><span class="math display">\[
Var(\epsilon)=
\left(
\begin{array}{c c c c c}
Var(\epsilon_{1}) &amp; Cov(\epsilon_{1},\epsilon_{2}) &amp; Cov(\epsilon_{1},\epsilon_{3}) &amp; \ldots &amp; Cov(\epsilon_{1},\epsilon_{ n})\\
Cov(\epsilon_{2},\epsilon_{1})&amp;Var(\epsilon_{2}) &amp; Cov(\epsilon_{2},\epsilon_{3}) &amp; \ldots &amp;Cov(\epsilon_{2},\epsilon_{n}) \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
Cov(\epsilon_{n},\epsilon_{1})   &amp; Cov(\epsilon_{n},\epsilon_{2}) &amp; Cov(\epsilon_{n},\epsilon_{3}) &amp; \ldots &amp; Var(\epsilon_{n})\\
\end{array}
\right)
\]</span></p>
<p>Por <strong>definición 3.1</strong>, <span class="math inline">\(Cov(\epsilon_{i},\epsilon_{j})=0 \ \ \ i\neq j\)</span></p>
<p><span class="math display">\[
Var(\epsilon)=
\left(
\begin{array}{c c c c c}
Var(\epsilon_{1}) &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0&amp;Var(\epsilon_{2}) &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; Var(\epsilon_{n})\\
\end{array}
\right)
\]</span></p>
<p>Por <strong>definición 3.1</strong>, <span class="math inline">\(Var(\epsilon_{i})=\sigma^2\)</span></p>
<p><span class="math display">\[
Var(\epsilon)=
\left(
\begin{array}{c c c c c}
\sigma^2 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0&amp;\sigma^2 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; 0 &amp; \ldots &amp;\sigma^2  \\
\end{array}
\right)
\]</span></p>
<p><span class="math inline">\(\therefore Var(\epsilon)=\sigma^2 I_{n \times n}. \blacksquare\)</span></p>
<div id="estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-2" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Estimación por mínimos cuadrados de los parámetros del modelo<a href="modelo.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Es necesario dar una estimación de la intersección con el eje <span class="math inline">\(\underline{Y}\)</span>, las variables que conforman el hiperplano, es decir, <span class="math inline">\(\beta_{0},\beta_{1},\beta_{2},\ldots,\beta_{k}\)</span> respectivamente. La manera en la que se construyen a los estimadores es tal que la diferencia entre todos los valores observados y los valores estimados sea 0, es decir, a éstas diferencias se le conoce como <span class="math inline">\(\mathbf{residuales}\)</span>, muchos autores también hacen referencia a ellos como <span class="math inline">\(\mathbf{residuos}\)</span>.</p>
<p><strong>Definición 3.3</strong> (Residuales). Sea <span class="math inline">\(y_{i}\)</span> los valores observados, y sea <span class="math inline">\(\hat{y}_{i}\)</span> los valores ajustados de la forma <span class="math inline">\(\hat{y}_{i}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{1i}+\hat{\beta_{2}}x_{2i}+ \ldots +\hat{\beta_{k}}x_{ki}\)</span>  para  <span class="math inline">\(\hat{\beta_{0}},\hat{\beta_{1}},\hat{\beta_{2}}, \ldots, \hat{\beta_{k}}\)</span>  dados,  entonces:</p>
<p><span class="math display">\[e_{i}=y_{i}-\hat{y_{i}} \ \ \ \ \ \ i=1, \ldots,n.\]</span>
Se les conoce como <strong>residuales.</strong></p>
<p>Para estimar los valores desconocidos <span class="math inline">\(\beta_{0},\beta_{1},\beta_{2},\ldots,\beta_{k}\)</span> se usa el <strong>método de mínimos cuadrados</strong>, el cual es similar al caso de regresión lineal simple, dicho método propone minimizar la suma de cuadrados de los residuales.</p>
<p>Antes de continuar es necesario ver algunos resultados importantes de equivalencia y notación.</p>
<p>De la <strong>definición 3.3</strong> se sabe que los valores esperados de <span class="math inline">\(y_{i}\)</span> pueden ser definidos como:</p>
<p><span class="math display">\[
\begin{array}{c}
\hat{y_{1}} \\
\hat{y_{2}} \\
\vdots \\
\hat{y_{n}}\\
\end{array}
\begin{array}{c c c c c c c c c c}
=&amp;\hat{\beta_{0}}&amp;+&amp;\hat{\beta_{1}} x_{11} &amp;+&amp; \hat{\beta_{2}}x_{12}&amp;+&amp; \ldots &amp;+&amp;\hat{\beta_{k}} x_{1k}\\
=&amp;\hat{\beta_{0}}&amp;+&amp;\hat{\beta_{1}} x_{21} &amp;+&amp; \hat{\beta_{2}}x_{22}&amp;+&amp; \ldots &amp;+&amp;\hat{\beta_{k}} x_{2k}\\
&amp;\vdots &amp;&amp; \vdots &amp;&amp; \vdots &amp;&amp; \vdots &amp;&amp; \vdots\\
=&amp;\hat{\beta_{0}}&amp;+&amp;\hat{\beta_{1}} x_{n1} &amp;+&amp; \hat{\beta_{2}}x_{n2}&amp;+&amp; \ldots &amp;+&amp;\hat{\beta_{k}} x_{nk}\\
\end{array}
\]</span></p>
<p>La anterior ecuación puede ser descompuesta en forma matricial de la siguiente manera:</p>
<p><span class="math display">\[
\left(
\begin{array}{c}
\hat{y_{1}} \\
\hat{y_{2}} \\
\vdots \\
\hat{y_{n}}\\
\end{array}
\right)=
\left(
\begin{array}{c c c c c}
1      &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1k}\\
1      &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1      &amp; x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{nk}\\
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta_{0}} \\
\hat{\beta_{1}} \\
\vdots \\
\hat{\beta_{k}}\\
\end{array}
\right)
\]</span></p>
<p>Por lo tanto, podemos renombrar a las matrices de acuerdo a los elementos que las conforman.</p>
<p>Se tiene la siguiente igualdad para los valores estimados <span class="math inline">\(\underline{\hat{Y}}.\)</span></p>
<p><span class="math display">\[\underline{\hat{Y}}=X \underline{\hat{\beta}}.\]</span></p>
<p>Ahora por la <strong>definición 3.3</strong> y lo anterior tenemos que los residuales se encuentran de la forma:</p>
<p><span class="math display">\[\underline{e}=\underline{Y}-X \underline{\hat{\beta}}.\]</span></p>
<p><strong>Teorema 3.2</strong> (Mínimos Cuadrados).(MC) Si se minimiza la suma de cuadrados de la diferencia entre los valores observados y los estimados, la cual se expresa matricialmente de la siguiente forma:</p>
<p><span class="math display">\[\underline{e&#39;}\underline{e}.\]</span></p>
<p>Entonces se tiene como estimador de <span class="math inline">\(\underline{\beta}\)</span> a:</p>
<p><span class="math display">\[\underline{\hat{\beta}}=\left( X&#39;X\right)^{-1}X&#39;\underline{Y}.\]</span></p>
<p><strong>Demostración:</strong></p>
<p>Se sabe que los residuales están definidos como <span class="math inline">\(\underline{e}=\left( \underline{Y}-X \underline{\hat{\beta}}\right)\)</span> de esta manera, por hipótesis se tiene:</p>
<p><span class="math display">\[\underline{e&#39;}\underline{e}=\left( \underline{Y}-X \underline{\hat{\beta}}\right)&#39;\left( \underline{Y}-X \underline{\hat{\beta}}\right).\]</span>
<span class="math display">\[=\left(\underline{Y}&#39;- \underline{\hat{\beta}}&#39;X&#39;\right)\left( \underline{Y}-X \underline{\hat{\beta}}\right).\]</span>
<span class="math display">\[=\underline{Y}&#39;Y-\underline{Y}&#39;X\underline{\hat{\beta}}-\underline{\hat{\beta}}&#39;X&#39;\underline{Y}+\underline{\hat{\beta}}&#39;X&#39;X\underline{\hat{\beta}}.\]</span>
<span class="math display">\[\underline{e&#39;}\underline{e}=\underline{Y}&#39;Y-2\underline{Y}&#39;X\underline{\hat{\beta}}+\underline{\hat{\beta}}&#39;X&#39;X\underline{\hat{\beta}}.\]</span>
Lo anterior se da ya que <span class="math inline">\(\underline{\hat{\beta}}&#39;X&#39;\underline{Y}\)</span> es una matriz de <span class="math inline">\(1\times 1\)</span>, es decir, un escalar, y que su transpuesta <span class="math inline">\((\underline{\hat{\beta}}&#39;X&#39;\underline{Y})&#39;=\underline{Y}&#39;X\underline{\hat{\beta}}\)</span> es el mismo escalar.</p>
<p>Derivando respecto a <span class="math inline">\(\underline{\hat{\beta}}\)</span> para hallar los posibles mínimos se divide la suma matricial de la siguiente forma:</p>
<p><span class="math display">\[\Delta \underline{\hat\beta}=\Delta_{1} \underline{\hat{\beta}}+\Delta_{2} \underline{\hat{\beta}}+\Delta_{3} \underline{\hat{\beta}}\]</span></p>
<p>Procederemos a derivar:</p>
<p><span class="math display">\[
\Delta\underline{\hat{\beta}}=\left\{
\begin{array}{ll}
\Delta_{1} \underline{\hat{\beta}} \ \ \ \ \ \left(\underline{Y}&#39;\underline{Y}\right)  \ \ \ \ \ \ \ \ =0 \\
\Delta_{2} \underline{\hat{\beta}} \ \ \ \ \left( -2\underline{Y}&#39;X\underline{\hat{\beta}}\right) \ \ = \left(-2\underline{Y}&#39;X\right)&#39;=-2X&#39;\underline{Y} \\
\Delta_{3} \underline{\hat{\beta}} \ \  \ \ \left( \underline{\hat{\beta}&#39;}X&#39;X\underline{\hat{\beta}}\right)  \ \ \ = 2X&#39;X\underline{\hat{\beta}}
\end{array}
\right.
\]</span></p>
<p>De esta forma se tiene que la derivada respecto a <span class="math inline">\(\underline{\beta}\)</span> es:</p>
<p><span class="math display">\[\Delta \underline{\beta}=-2X&#39;\underline{Y}+2X&#39;X\underline{\hat{\beta}}.\]</span>
Igualamos la derivada a 0, para hallar un punto crítico:</p>
<p><span class="math display">\[\Delta \underline{\hat{\beta}}=0\]</span>
<span class="math display">\[-2X&#39;\underline{Y}+2X&#39;X\underline{\hat{\beta}}=0\]</span>
<span class="math display">\[2X&#39;X\underline{\hat{\beta}}=2X&#39;\underline{Y}\]</span>
Esto se simplifica a:</p>
<p><span class="math display">\[X&#39;X\underline{\hat{\beta}}=X&#39;\underline{Y}\]</span>
Ahora multiplicamos ambos lados por la inversa de <span class="math inline">\(X&#39;X\)</span>, es decir, <span class="math inline">\((X&#39;X)^{-1}\)</span></p>
<p><span class="math display">\[\therefore \  \underline{\hat{\beta}}=\left(X&#39;X\right)^{-1}X&#39;\underline{Y}.\]</span>
Nótese que la inversa <span class="math inline">\(\left(X&#39;X\right)^{-1}\)</span> existe porque <span class="math inline">\(X\)</span> es de rango completo en las columnas, como se mencionó en la definición. Por lo que el producto matricial <span class="math inline">\(X&#39;X\)</span> es de rango completo (<span class="math inline">\(k+1\)</span>), es decir <span class="math inline">\(|X|\neq 0,\)</span> por lo que se garantiza la existencia de la inversa.</p>
<p>Realizando la segunda derivada, veremos si la función es cóncava o convexa para saber si es mínimo o máximo.</p>
<p><span class="math display">\[
\Delta \Delta \underline{\hat{\beta}}=\left\{
\begin{array}{ll}
\Delta \Delta \underline{\hat{\beta}} \ \ \ \ -2X&#39; \underline{Y} \ \ =0 \\
\Delta \Delta \underline{\hat{\beta}} \ \ \ \ 2X&#39;X \underline{\hat{\beta}}  \ \ \ = \left(2X&#39;X\right)&#39; =2X&#39;X \\
\end{array}
\right.
\]</span></p>
<p>Así <span class="math inline">\(\Delta \Delta \underline{\hat{\beta}}=2X&#39;X\)</span></p>
<p>De esta manera <span class="math inline">\(\Delta \Delta \underline{\hat{\beta}}&gt;0\)</span> ya que <span class="math inline">\(X&#39;X\)</span> es definida positiva , por consiguiente <span class="math inline">\((X&#39;X)^{-1}X&#39;\underline{Y}\)</span> es considerado un mínimo. Por lo tanto, <span class="math inline">\(\underline{\hat{\beta}}=\left(X&#39;X\right)^{-1}X&#39;\underline{Y}.\)</span> es el estimador de mínimos cuadrados del modelo de regresión múltiple.<span class="math inline">\(\blacksquare\)</span></p>
<p>Una vez encontrada una estimación a los parámetros desconocidos de <span class="math inline">\(\underline{\beta}\)</span>, será conveniente desarrollar algunas variantes en la forma en la que se denota a los residuales, para ello se define a la matriz <span class="math inline">\(H\)</span> como <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;.\)</span> Cabe destacar que la matriz <span class="math inline">\(H\)</span> es conocida como <strong>“matriz sombrero”</strong>, que junto con la matriz <span class="math inline">\((I-H)\)</span> cumplen con ser matrices idempotentes, es decir, que al elevar las matrices a una potencia dada los valores contenidos en la matriz no se modifican; de igual forma ambas matrices cumplen con ser simétricas, denominadas así ya que al transponer las matrices los valores contenidos en ellas conservan su lugar.</p>
<p>Debemos considerar el siguiente resultado, el cual será importante al desarrollar el siguiente <strong>teorema 3.3</strong> ya que demuestra que <span class="math inline">\((X&#39;X)^{-1}\)</span> es una matriz simétrica.</p>
<p><span class="math display">\[[(X&#39;X)^{-1}]&#39;=[(X&#39;X)&#39;]^{-1}\]</span>
<span class="math display">\[=(X&#39;(X&#39;)&#39;)^{-1}\]</span>
<span class="math display">\[\therefore \  [(X&#39;X)^{-1}]&#39;= (X&#39;X)^{-1}. \ \blacksquare\]</span>
Es decir, la inversa de <span class="math inline">\(X&#39;X\)</span> es simétrica, resultado importante en el siguiente teorema:</p>
<p><strong>Teorema 3.3</strong> Sea <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;\)</span> e <span class="math inline">\((I-H)\)</span> entonces:</p>
<p><strong>a)</strong> Las matrices <span class="math inline">\(H\)</span> e <span class="math inline">\(I-H\)</span> son idempotentes.</p>
<p><strong>b)</strong> Las matrices <span class="math inline">\(H\)</span> e <span class="math inline">\(I-H\)</span> son simétricas.</p>
<p><strong>Demostración:</strong></p>
<p><strong>a)</strong> Para demostrar la idempotencia de <span class="math inline">\(H\)</span> basta probar que <span class="math inline">\(H^2=H,\)</span> es decir, al elevar la matriz <span class="math inline">\(H\)</span> ésta no se alterará:</p>
<p><span class="math display">\[H^2=(X(X&#39;X)^{-1}X&#39;)(X(X&#39;X)^{-1}X&#39;)\]</span>
<span class="math display">\[=X(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}X&#39;.\]</span>
Transponiendo con la finalidad de simplificar el producto matricial y por el resultado mostrado anteriormente <span class="math inline">\([(X&#39;X)^{-1}]&#39;=(X&#39;X)^{-1}\)</span> se tiene:</p>
<p><span class="math display">\[=[(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}X&#39;]&#39;X&#39;\]</span>
<span class="math display">\[=[(X&#39;X)^{-1}X&#39;]&#39;X&#39;\]</span>
<span class="math display">\[=X(X&#39;X)^{-1}X&#39;\]</span></p>
<p><span class="math display">\[\therefore H^2=H.\]</span>
Por lo tanto <span class="math inline">\(H\)</span> es idempotente. <span class="math inline">\(\blacksquare\)</span></p>
<p>Para probar la idempotencia de <span class="math inline">\(I-H\)</span>, ésta será elevada al cuadrado.</p>
<p><span class="math display">\[(I-H)^2=(I-H)(I-H)\]</span>
<span class="math display">\[=I-IH-IH+H^2\]</span>
<span class="math display">\[=I-2H+H^2.\]</span></p>
<p>Por idempotencia de <span class="math inline">\(H\)</span>, <span class="math inline">\(H=H^2\)</span>. Por lo tanto:
<span class="math display">\[(I-H)=I-2H+H\]</span>
<span class="math display">\[\therefore (I-H)^2=I-H.\]</span></p>
<p><strong>b)</strong> Para demostrar la simetría de <span class="math inline">\(H\)</span>, se transpondrá la matriz <span class="math inline">\(H\)</span>. Además debemos recordar que <span class="math inline">\([(X&#39;X)^{-1}]&#39;=(X&#39;X)^{-1}\)</span> así:</p>
<p><span class="math display">\[H&#39;= (X(X&#39;X)^{-1}X&#39;)&#39;\]</span>
<span class="math display">\[=X(X&#39;X)^{-1}X&#39;\]</span>
<span class="math display">\[\therefore H&#39;= H.\]</span></p>
<p>Por lo tanto la matriz <span class="math inline">\(H\)</span> es simétrica.</p>
<p>Para la simetría de <span class="math inline">\(I-H\)</span> se transpone la matriz:</p>
<p><span class="math display">\[(I-H)&#39;=I&#39;-H&#39;.\]</span></p>
<p>Por simetría de <span class="math inline">\(H\)</span> y de <span class="math inline">\(I\)</span></p>
<p><span class="math display">\[\therefore (I-H)^2=I-H\]</span></p>
<p>Por lo tanto <span class="math inline">\(I-H\)</span> es simétrica. <span class="math inline">\(\blacksquare\)</span></p>
<p><strong>Corolario 4</strong> Sea <span class="math inline">\(\underline{e}\)</span> la matriz de residuales, entonces éstos pueden ser expresados por la siguiente ecuación:</p>
<p><span class="math display">\[\underline{e}=(I-H)\underline{Y}\]</span>
donde <span class="math inline">\(I\)</span> es la matriz identidad, y <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;.\)</span></p>
<p><strong>Demostración:</strong></p>
<p>Se sabe que los valores estimados son calculados de la siguiente manera:</p>
<p><span class="math display">\[\underline{\hat{Y}}=X\underline{\hat{\beta}}\]</span>
<span class="math display">\[\underline{\hat{Y}}=X(X&#39;X)^{-1}X&#39;\underline{Y}\]</span>
<span class="math display">\[\underline{\hat{Y}}=H\underline{Y}.\]</span>
donde <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;.\)</span> De esta manera calculando la matriz de residuales se tiene:</p>
<p><span class="math display">\[\underline{e}=\underline{Y}-\underline{\hat{Y}}\]</span>
<span class="math display">\[\underline{e}=\underline{Y}-X\underline{\hat{\beta}}\]</span>
<span class="math display">\[\underline{e}=\underline{Y}-H\underline{Y}\]</span>
<span class="math display">\[\underline{e}=(I-H)\underline{Y}.\blacksquare\]</span></p>
<p>Como se mencionó en regresión lineal simple, <span class="math inline">\(SC_{error}\)</span> mide la variación residual que queda sin explicar por la línea de regresión, en el modelo de regresión múltiple es denotada como <span class="math inline">\(SC_{error}=\underline{e}&#39;\underline{e},\)</span> la cual es equivalente a la suma de residuales al cuadrado.</p>
<p><strong>Corolario 5</strong> La suma de cuadrados del error, puede denotarse matricialmente como:</p>
<p><span class="math display">\[SC_{error}=\underline{Y}&#39;(I-H)\underline{Y}.\]</span>
donde:</p>
<ul>
<li><p><span class="math inline">\(&quot;\underline{Y}&quot;\)</span> son los valores observados de la variable respuesta.</p></li>
<li><p><span class="math inline">\(I\)</span> es la matriz identidad.</p></li>
<li><p><span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;.\)</span></p></li>
</ul>
<p><strong>Demostración:</strong></p>
<p>Se sabe por hipótesis que:</p>
<p><span class="math inline">\(SC_{error}=\underline{e}&#39;\underline{e}\)</span></p>
<p>Por el <strong>corolario 4</strong>, se puede expresar a los residuales como <span class="math inline">\(\underline{e}=(I-H)\underline{Y},\)</span> sustituyendo:</p>
<p><span class="math display">\[SC_{error}=((I-H)\underline{Y})&#39;((I-H)\underline{Y})\]</span></p>
<p><span class="math display">\[=(\underline{Y}&#39;(I-H)&#39;)((I-H)\underline{Y})\]</span></p>
<p><span class="math display">\[=(\underline{Y}&#39;(I-H))((I-H)\underline{Y})\]</span></p>
<p><span class="math display">\[=\underline{Y}&#39;(I-H)(I-H)\underline{Y}\]</span></p>
<p><span class="math display">\[=[(I-H)&#39;(I-H)&#39;\underline{Y}]\underline{Y}\]</span></p>
<p><span class="math display">\[=[(I-H)^2\underline{Y}]&#39;\underline{Y}\]</span></p>
<p><span class="math display">\[=\underline{Y}&#39;(I-H)&#39;\underline{Y}\]</span></p>
<p><span class="math display">\[\therefore SC_{error}=\underline{Y}&#39;(I-H)&#39;\underline{Y}. \blacksquare\]</span></p>
<p>Con los resultados, se procede a examinar las propiedades de los estimadores obtenidos por el método de mínimos cuadrados.
Éstas propiedades son agrupadas y enunciadas en el <strong>Teorema de Gauss-Markov.</strong></p>
<p><strong>Teorema 3.4</strong> (Teorema de Gauss-Markov).</p>
<p>En el modelo de <strong>regresión lineal múltiple</strong> <span class="math inline">\(\underline{Y}=X \underline{\beta}+ \underline{\epsilon},\)</span> bajo la hipótesis:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{E}[\underline{\epsilon}]=0\)</span> y <span class="math inline">\(Var(\underline{\epsilon})=\sigma^2I_{n}.\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{E}[\underline{Y}]=X \underline{\beta}\)</span> y <span class="math inline">\(Var(\underline{Y})=\sigma^2I_{n}.\)</span></p></li>
<li><p>X de rango completo en las columnas.</p></li>
</ul>
<p>El estimador de mínimos cuadrados de <span class="math inline">\(\underline{\beta}\)</span>, es el <strong>MELI</strong> (<strong>BLUE</strong> por su abreviación en inglés), el mejor estimador lineal insesgado. Es decir, <span class="math inline">\(\underline{\hat{\beta}}\)</span> es insesgado y además, si <span class="math inline">\(\underline{\tilde{\beta}}\)</span> es otro estimador insesgado, entonces <span class="math inline">\(Var(\underline{\tilde{\beta}})\geq Var(\underline{\hat{\beta}})\)</span>, es decir, <span class="math inline">\(\underline{\hat{\beta}}\)</span> es de mínima varianza.</p>
<p><strong>Demostración:</strong></p>
<p>Para demostrar que el estimador <span class="math inline">\(\underline{\hat{\beta}}\)</span> es insesgado, es necesario probar que el estimador cumple que <span class="math inline">\(\mathbf{E}[\underline{\hat{\beta}}]=\underline{\beta}\)</span>, de ésta forma:</p>
<p><span class="math display">\[\mathbf{E}[\underline{\hat{\beta}}]=\mathbf{E}[(X&#39;X)^{-1}X&#39;\underline{Y}]\]</span>
Ya que <span class="math inline">\(X\)</span> son constantes</p>
<p><span class="math display">\[=(X&#39;X)^{-1}X&#39;\mathbf{E}[\underline{Y}]\]</span>
Ya que <span class="math inline">\(\mathbf{E}[\underline{Y}]=X\underline{\beta}\)</span></p>
<p><span class="math display">\[=(X&#39;X)^{-1}X&#39;X\beta\]</span>
<span class="math display">\[=I\beta\]</span></p>
<p><span class="math display">\[\therefore \mathbf{E}[\underline{\hat{\beta}}]=\underline{\beta}.\]</span>
Por lo tanto <span class="math inline">\(\underline{\hat{\beta}}\)</span> es un estimador insesgado para <span class="math inline">\(\underline{\beta}\)</span>.<span class="math inline">\(\blacksquare\)</span></p>
<p>Para conocer la varianza del estimador <span class="math inline">\(\underline{\hat{\beta}}\)</span> se sabe que:</p>
<p><span class="math display">\[Var(\underline{\hat{\beta}})=Var\left( (X&#39;X)^{-1}X&#39;\underline{Y}\right)\]</span>
<span class="math display">\[=(X&#39;X)^{-1}X&#39;Var(\underline{Y})[(X&#39;X)^{-1}X&#39;]&#39;\]</span>
Ya que la <span class="math inline">\(Var(\underline{Y})=\sigma^2I_{n}\)</span></p>
<p><span class="math display">\[=(X&#39;X)^{-1}X&#39;[(X&#39;X)^{-1}X&#39;]&#39;\sigma^2\]</span>
<span class="math display">\[=\sigma^2(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}\]</span>
<span class="math display">\[
=\sigma^2I(X&#39;X)^{-1}
\]</span>
<span class="math display">\[
\therefore Var(\underline{\hat{\beta}})=\sigma^2(X&#39;X)^{-1}
\]</span>
Para comprobar que el estimador <span class="math inline">\(\underline{\hat{\beta}}\)</span> es el estimador insesgado de mínima varianza, se propone a un estimador <span class="math inline">\(\underline{\tilde{\beta}}\)</span> el cual cumple con ser lineal e insesgado. Para ello sea <span class="math inline">\(\underline{\tilde{\beta}}\)</span> un estimador linealmente insesgado para <span class="math inline">\(\underline{\beta}\)</span>. Es decir, existe una matriz de <span class="math inline">\(A_{(k+1)\times n}\)</span> tal que <span class="math inline">\(\underline{\tilde{\beta}}=A \underline{Y}.\)</span>
De esta forma:</p>
<p><span class="math display">\[
\mathbf{E}\left[\underline{\tilde{\beta}}\right]=\mathbf{E}\left[ A\underline{Y} \right]
\]</span>
<span class="math display">\[
=A \mathbf{E}[\underline{Y}]
\]</span>
Ya que <span class="math inline">\(\mathbf{E}[\underline{Y}]=X\underline{\beta}\)</span></p>
<p><span class="math display">\[
=AX\underline{\beta}
\]</span>
<span class="math display">\[
\mathbf{E}\left[\underline{\tilde{\beta}}\right]=AX\underline{\beta}
\]</span></p>
<p>Para que sea un estimador insesgado, entonces <span class="math inline">\(AX\)</span> tiene que cumplir: <span class="math inline">\(AX=I\)</span>, así:</p>
<p><span class="math display">\[
\mathbf{E}\left[\underline{\tilde{\beta}}\right]=I\underline{\beta}
\]</span></p>
<p><span class="math display">\[
\therefore \mathbf{E}\left[\underline{\tilde{\beta}}\right]=\underline{\beta}.\blacksquare
\]</span></p>
<p>Para conocer la varianza de <span class="math inline">\(\underline{\tilde{\beta}}\)</span> se tiene:</p>
<p><span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)= Var(A\underline{Y})
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=A Var(\underline{Y})A&#39;
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2 AA&#39;.
\]</span>
Sea <span class="math inline">\(C\)</span> una matriz de dimensión <span class="math inline">\((k+1) \times n\)</span> tal que <span class="math inline">\(C=A-(X&#39;X)^{-1}X&#39;\)</span>. Observe que <span class="math inline">\(CX=0\)</span> ya que <span class="math inline">\(CX=AX-(X&#39;X)^{-1}X&#39;X=I-I=0.\)</span>
De esta forma se tiene la siguiente igualdad:</p>
<p><span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2((X&#39;X)^{-1}X&#39;+C)((X&#39;X)^{-1}X&#39;+C)&#39;
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2((X&#39;X)^{-1}X&#39;+C)\left[ ((X&#39;X)^{-1}X&#39;)&#39;+C&#39;\right]
\]</span></p>
<p><span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2((X&#39;X)^{-1}X&#39;+C)[X(X&#39;X)^{-1}+C&#39;]
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2[(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}+(X&#39;X)^{-1}X&#39;C&#39;+CX(X&#39;X)^{-1}+CC&#39;]
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2\left[(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}+[CX(X&#39;X)^{-1}]&#39;+CX(X&#39;X)^{-1}+CC&#39;\right]
\]</span>
Debido a que <span class="math inline">\(CX=0\)</span></p>
<p><span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2\left[ I(X&#39;X)^{-1}+0+0+CC&#39;\right]
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2[(X&#39;X)^{-1}+CC&#39;]
\]</span>
<span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right)=\sigma^2(X&#39;X)^{-1}+\sigma^2CC&#39;.
\]</span></p>
<p><span class="math display">\[
\therefore \  Var\left(\underline{\tilde{\beta}}\right)=Var\left(\underline{\hat{\beta}}\right)+\sigma^2CC&#39;. \blacksquare
\]</span></p>
<p>Además se observa que <span class="math inline">\(CC&#39;\)</span>, es una matriz semidefinida positiva ya que los valores propios de <span class="math inline">\(CC&#39;\)</span> son reales y no negativos, además debido al supuesto de que <span class="math inline">\(X\)</span> es de rango completo para las columnas se cumple que <span class="math inline">\(rango(CC&#39;)=rango(X)=k+1,\)</span> esto es importante, ya que si no se cumple se tendría una solución no trivial, por lo que <span class="math inline">\(0\)</span> podría ser una solución para un egeinvalor por lo que no sería semidefinido positivo. Como <span class="math inline">\(CC&#39;\geq0\)</span>, se observa que:</p>
<p><span class="math display">\[
Var\left(\underline{\tilde{\beta}}\right) \geq Var\left(\underline{\hat{\beta}}\right).
\]</span>
Por lo que la varianza del estimador propuesto es mayor al obtenido por mínimos cuadrados. Por lo tanto, el estimador de <span class="math inline">\(MC\)</span> de <span class="math inline">\(\underline{\beta}\)</span> es el mejor estimador linealmente insesgado y de mínima varianza.<span class="math inline">\(\blacksquare\)</span></p>
<p>Las anteriores propiedades de los estimadores son importantes ya que garantizan que los valores estimados <span class="math inline">\(\underline{\hat{Y}}\)</span>, asignan valores que efectivamente recaen en el hiperplano propuesto en el modelo de regresión lineal múltiple.</p>
<p><strong>Teorema 3.5</strong> Sea <span class="math inline">\(\underline{\hat{Y}}\)</span>, los valores estimados de <span class="math inline">\(Y\)</span>, de forma que <span class="math inline">\(\underline{\hat{Y}}=X\underline{\hat{\beta}}\)</span>, entonces se cumple:</p>
<p><strong>a)</strong> <span class="math inline">\(\mathbf{E}[\underline{\hat{Y}}]=X\underline{\beta}.\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(\textbf{Var}(\underline{\hat{Y}})=\sigma^2H.\)</span></p>
<p><strong>Demostración:</strong></p>
<p><strong>a)</strong> Para demostrar la esperanza de los valores estimados, se observa que:</p>
<p><span class="math display">\[
\mathbf{E}[\underline{\hat{Y}}]=\mathbf{E}[X\underline{\hat{\beta}}]
\]</span>
<span class="math display">\[
\mathbf{E}[\underline{\hat{Y}}]=X\mathbf{E}[\underline{\hat{\beta}}]
\]</span></p>
<p><span class="math display">\[
\therefore \mathbf{E}[\underline{\hat{Y}}]=X\underline{\beta}. \blacksquare
\]</span></p>
<p><strong>b)</strong> Para la varianza se tiene:</p>
<p><span class="math display">\[
Var(\underline{\hat{Y}})=Var(X\underline{\hat{\beta}})
\]</span>
<span class="math display">\[
Var(\underline{\hat{Y}})=XVar(\underline{\hat{\beta}})X&#39;
\]</span>
<span class="math display">\[
Var(\underline{\hat{Y}})=X\sigma^2(X&#39;X)^{-1}X&#39;
\]</span></p>
<p><span class="math display">\[
Var(\underline{\hat{Y}})=\sigma^2X(X&#39;X)^{-1}X&#39;
\]</span></p>
<p><span class="math display">\[
\therefore Var(\underline{\hat{Y}})=\sigma^2H.\blacksquare
\]</span></p>
<p><strong>Teorema 3.6</strong> Sea <span class="math inline">\(\underline{e}\)</span> los residuales del modelo, de forma <span class="math inline">\(\underline{e}=\underline{Y}-\underline{\hat{Y}}\)</span>, entonces cumplen con:</p>
<p><strong>a)</strong> <span class="math inline">\(\mathbf{E}[\underline{e}]=0.\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(\textbf{Var}(\underline{e})=\sigma^2(I-H).\)</span></p>
<p><strong>Demostración:</strong></p>
<p><span class="math inline">\(**a)**\)</span> Para demostar la esperanza de los residuales, se observa que:</p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=\mathbf{E}[\underline{Y}-\underline{\hat{Y}}]
\]</span>
Por el <strong>corolario 4</strong></p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=\mathbf{E}[(I-H)\underline{Y}]
\]</span>
<span class="math display">\[
\mathbf{E}[\underline{e}]=(I-H)\mathbf{E}[\underline{Y}]
\]</span></p>
<p>Por el <strong>teorema 3.4</strong></p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=(I-H)X\beta
\]</span></p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=X\underline{\beta}-HX\underline{\beta}
\]</span>
<span class="math display">\[
\mathbf{E}[\underline{e}]=X\underline{\beta}-X(X&#39;X)^{-1}X&#39;X\underline{\beta}
\]</span>
<span class="math display">\[
\mathbf{E}[\underline{e}]=X\underline{\beta}-\left[X&#39;X(X&#39;X)^{-1}X&#39;\right]&#39;\underline{\beta}
\]</span></p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=X\underline{\beta}-[IX&#39;]&#39;\underline{\beta}
\]</span></p>
<p><span class="math display">\[
\mathbf{E}[\underline{e}]=X\underline{\beta}-X\underline{\beta}
\]</span>
<span class="math display">\[
\therefore \mathbf{E}[\underline{e}]=0. \blacksquare
\]</span></p>
<p><strong>b)</strong> Para la varianza se tiene:</p>
<p><span class="math display">\[
Var(\underline{e})=Var\left( \underline{Y}-\underline{\hat{Y}}\right)
\]</span></p>
<p>Por el <strong>corolario 4</strong></p>
<p><span class="math display">\[
Var(\underline{e})=Var\left( (I-H)\underline{\hat{Y}}\right)
\]</span>
<span class="math display">\[
Var(\underline{e})=(I-H)Var(\underline{\hat{Y}})(I-H)&#39;
\]</span>
Por el <strong>teorema 3.3</strong></p>
<p><span class="math display">\[
Var(\underline{e})=(I-H)\sigma^2(I-H)
\]</span>
Por idempotencia de <span class="math inline">\(I-H\)</span></p>
<p><span class="math display">\[
Var(\underline{e})=\sigma^2(I-H)(I-H)
\]</span></p>
<p><span class="math display">\[
\therefore Var(\underline{e})=\sigma^2(I-H). \blacksquare
\]</span></p>
</div>
<div id="estimación-por-máxima-verosimilitud" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Estimación por máxima verosimilitud<a href="modelo.html#estimación-por-máxima-verosimilitud" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Se han usado varios supuestos para poder calcular a los estimadores por medio del método de mínimos cuadrados, sin embargo, para hacer uso de la estimación por máxima verosimilitud se supondrá que los errores se distribuyen como una normal multivariada <span class="math inline">\(\underline{\epsilon} \sim \mathbf{N}_{n}(O_{n},\sigma^2 I_{n})\)</span> por lo que el modelo <span class="math inline">\(\underline{Y}\)</span> tiene distribución normal, es decir, <span class="math inline">\(\underline{Y} \sim \mathbf{N}_n (X\underline{\beta},\sigma^2).\)</span></p>
<p>Tenemos:</p>
<p><strong>Definición 3.4</strong> Tomando el supuesto de normalidad conjunta para los errores se cumple que:</p>
<ul>
<li><p><span class="math inline">\(\epsilon_{i}\)</span> es independiente <span class="math inline">\(\forall i\)</span> tal que <span class="math inline">\(i \neq j.\)</span></p></li>
<li><p><span class="math inline">\(\underline{Y}\sim \mathbf{N}_{n}(X\underline{\beta},\sigma^2).\)</span></p></li>
<li><p>Cada <span class="math inline">\(y_{i}\)</span> es independiente pero no es idénticamente distribuida.</p></li>
</ul>
<p>De esta forma se podrá usar el método de máxima verosimilitud para estimar a los parámetros desconocidos a través de la función de verosimilitud. Al realizar la estimación por éste método se obtendrán resultados parecidos a los obtenidos por mínimos cuadrados.</p>
<p><strong>Teorema 3.7</strong> (Función de verosimilitud).</p>
<p>Sea <span class="math inline">\(\underline{\hat{\beta}}\)</span> y <span class="math inline">\(\hat{\sigma}^2\)</span>, los estimadores de <span class="math inline">\(\underline{\beta}\)</span> y <span class="math inline">\(\sigma^2\)</span> respectivamente; suponiendo normalidad en los errores <span class="math inline">\(\underline{\epsilon} \sim \mathbf{N}_{n}(0_{n},\sigma^2 I_{n})\)</span> y <span class="math inline">\(\underline{Y}\sim \mathbf{N}_{n}(X \underline{\beta},\sigma^2)\)</span> entonces la estimación de los parámetros <span class="math inline">\(\underline{\beta}\)</span> y <span class="math inline">\(\sigma^2\)</span> por el método de máxima verosimilitud están dados por:</p>
<p><strong>a)</strong> <span class="math inline">\(\underline{\hat{\beta}}=(X&#39;X)^{-1}X&#39;\underline{Y}\)</span>.</p>
<p><strong>b)</strong> <span class="math inline">\(\hat{\sigma}^2=\frac{1}{n}\left( \underline{Y}-X\underline{\hat{\beta}}\right)&#39;\left(\underline{Y}-X \underline{\hat{\beta}}\right).\)</span></p>
<p><strong>Demostración:</strong></p>
<p>Por hipótesis <span class="math inline">\(\underline{Y}\sim \mathbf{N}_{n}(X\underline{\beta},\sigma^2 I_{n})\)</span>, escribiendo la función de verosimilitud se tiene que:</p>
<p><span class="math display">\[
L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=\prod_{i=1}^{n}(2\pi\sigma^2)^{-1/2} exp \left[ - \frac{1}{2\sigma^2}(\underline{Y}-\mu)&#39;(\underline{Y}-\mu) \right]
\]</span></p>
<p><span class="math display">\[
L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=(2\pi\sigma^2)^{\sum_{i=1}^{n}-1/2} exp \left[ - \frac{1}{2\sigma^2}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta}) \right]
\]</span>
<span class="math display">\[
L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=(2\pi\sigma^2)^{-n/2} exp \left[ - \frac{1}{2\sigma^2}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta}) \right].
\]</span></p>
<p>Aplicando logaritmo natural a la función de verosimilitud:</p>
<p><span class="math display">\[
ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)= -\frac{n}{2}ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})
\]</span>
<span class="math display">\[ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)= -\frac{n}{2}[ln(2\pi)+ln(\sigma^2)]-\frac{1}{2\sigma^2}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})\]</span>
<span class="math display">\[ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)= -\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})\]</span></p>
<p><strong>a)</strong> Derivando respecto a <span class="math inline">\(\underline{\beta}\)</span> para obtener su estimador.</p>
<p><span class="math display">\[\frac{\partial}{\partial \underline{\beta}} \ ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=-\frac{1}{2\sigma^2}2(X&#39;X\underline{\beta}-X&#39;\underline{Y}).\]</span>
<span class="math display">\[\frac{\partial}{\partial \underline{\beta}} \ ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=-\frac{1}{\sigma^2}(X&#39;X\underline{\beta}-X&#39;\underline{Y}).\]</span></p>
<p>Igualando la derivada a 0, para encontrar el punto silla</p>
<p><span class="math display">\[\frac{\partial}{\partial \underline{\beta}} \ ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=0\]</span></p>
<p><span class="math display">\[-\frac{1}{\sigma^2}(X&#39;X\underline{\beta}-X&#39;\underline{Y})=0\]</span></p>
<p><span class="math display">\[(X&#39;X\underline{\beta}-X&#39;\underline{Y})=0\]</span></p>
<p><span class="math display">\[X&#39;X\underline{\beta}=X&#39;\underline{Y}\]</span></p>
<p><span class="math display">\[\therefore \underline{\hat{\beta}}=(X&#39;X)^{-1}X&#39;\underline{Y}. \blacksquare\]</span></p>
<p>Ya que <span class="math inline">\(X\)</span> es de rango completo por columnas entonces existe <span class="math inline">\((X&#39;X)^{-1}.\)</span></p>
<p><strong>b)</strong> Derivando respecto a <span class="math inline">\(\sigma^2\)</span> para obtener su estimador:</p>
<p><span class="math display">\[\frac{\partial}{\partial\sigma^2} \ ln \ L(\underline{\beta},\sigma^2 \mid \underline{Y},X)=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta}).\]</span></p>
<p>Igualando la derivada parcial a 0 para hallar el punto crítico de un posible máximo.</p>
<p><span class="math display">\[\frac{\partial}{\partial\sigma^2} \ ln \ L(\underline{\beta}, \sigma^2 \mid \underline{Y},X)=0\]</span>
<span class="math display">\[-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})=0\]</span>
<span class="math display">\[\frac{1}{2\sigma^4}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})=\frac{n}{2\sigma^2}\]</span>
<span class="math display">\[(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})=n\sigma^2\]</span>
<span class="math display">\[\therefore \hat{\sigma}^2=\frac{1}{n}\left( \underline{Y}-X\underline{\hat{\beta}}\right)&#39;\left(\underline{Y}-X \underline{\hat{\beta}}\right).\blacksquare\]</span></p>
<p>Por lo tanto los estimadores de máxima verosimilitud son:</p>
<p><span class="math display">\[
\begin{array}{c}
\underline{\hat{\beta}}=(X&#39;X)^{-1}X&#39;\underline{Y} \\
y \\
\hat{\sigma}^2=\frac{1}{n}\left( \underline{Y}-X\underline{\hat{\beta}}\right)&#39;\left(\underline{Y}-X \underline{\hat{\beta}}\right) \blacksquare
\end{array}
\]</span></p>
<p>Se observa que el estimador de <span class="math inline">\(\underline{\beta}\)</span> obtenido por el método de mínimos cuadrados es similar al estimador por máxima verosimilitud, sin embargo, éste último aporta mayor información al proporcionar el estimador para la varianza del modelo <span class="math inline">\(\hat{\sigma}^2.\)</span></p>
<p>De igual forma el estimador <span class="math inline">\(\hat{\sigma}^2_{MV}\)</span> guarda cierta relación con la suma de cuadrados residuales ya que se tiene:</p>
<p><span class="math display">\[\hat{\sigma}^2_{MV}=\frac{1}{n}(\underline{Y}-X\underline{\beta})&#39;(\underline{Y}-X\underline{\beta})\]</span>
Por el <strong>corolario 4</strong></p>
<p><span class="math display">\[\hat{\sigma}^2_{MV}=\frac{1}{n}SC_{error}\]</span></p>
<p>Por el <strong>corolario 5</strong></p>
<p><span class="math display">\[\hat{\sigma}^2_{MV}=\frac{1}{n}\underline{e}&#39; \ \underline{e}\]</span>
<span class="math display">\[\hat{\sigma}^2_{MV}=\frac{1}{n}\underline{Y}&#39;(I-H)\underline{Y}. \blacksquare \]</span>
Cabe destacar, que el estimador de <span class="math inline">\(\underline{\beta}\)</span> por máxima verosimilitud hereda todas las propiedades que cumple el estimador de mínimos cuadrados del <strong>teorema 3.4</strong>, es decir, <span class="math inline">\(\underline{\hat{\beta}}\)</span> es insesgado y de mínima varianza, sin embargo, el método de máxima verosimilitud proporciona una estimación para <span class="math inline">\(\sigma^2\)</span> el cual cumple con tener sesgo <span class="math inline">\((\mathbf{E}[\sigma^2]\neq 0).\)</span></p>
<p><span class="math display">\[\mathbf{E}[\hat{\sigma}^2]=\mathbf{E}\left[\frac{1}{n}\sum_{i=1}^{n}(\underline{Y_{i}}-\underline{\hat{Y}})&#39;(\underline{Y_{i}}-\underline{\hat{Y}}) \right]\]</span></p>
<p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}\mathbf{E}\left[(\underline{Y_{i}}+X\underline{\beta}-X\underline{\beta}-X\underline{\hat{\beta}})&#39;(\underline{Y_{i}}+X\underline{\beta}-X\underline{\beta}-X\underline{\hat{\beta}}) \right]\]</span>
<span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}\left(\mathbf{E}[(\underline{Y_{i}}-X\underline{\beta})(\underline{Y_{i}}-X\underline{\beta})&#39;]-\mathbf{E}\left[(X\underline{\hat{\beta}}-X\underline{\beta})(X\underline{\hat{\beta}}-X\underline{\beta})&#39;\right] \right)\]</span>
<span class="math display">\[=\sigma^2-\frac{1}{n^2}\mathbf{E}\left[\frac{1}{n}\sum_{i=1}^{n}(\underline{Y_{i}}-\underline{\hat{Y}})(\underline{Y_{i}}-\underline{\hat{Y}})&#39;-\sum_{i=1}^{n}(\underline{Y_{i}}-\underline{\hat{Y}})(\underline{Y_{i}}-\underline{\hat{Y}})&#39;\right]\]</span></p>
<p><span class="math display">\[=\sigma^2-\frac{n^2-nk-n}{n}\sigma^2\]</span></p>
<p><span class="math display">\[\therefore \mathbf{E}[\hat{\sigma}^2]=\frac{n-k-1}{n}\sigma^2. \]</span></p>
<p>Por lo tanto el estimador <span class="math inline">\(\hat{\sigma}^2\)</span> no es insesgado.<span class="math inline">\(\blacksquare\)</span></p>
<p>El método de mínimos cuadrados no proporciona información acerca de la estimación de varianza del modelo <span class="math inline">\(\sigma^2\)</span>, es por ello que se propone al estimador:</p>
<p><span class="math display">\[\sigma^2_{MC}=\frac{SC_{error}}{n-k-1}.\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introducción-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervalos-de-confianza-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "github", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
