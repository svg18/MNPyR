<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 1 Modelo con intercepto | Modelos de Regresión</title>
  <meta name="description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 1 Modelo con intercepto | Modelos de Regresión" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 1 Modelo con intercepto | Modelos de Regresión" />
  
  <meta name="twitter:description" content="Material para el curso Modelos no paramétricos y de regresión en la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />
<meta name="author" content="Dulce María Reyes Varela" />
<meta name="author" content="Carlos Fernando Vásquez Guerra" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducción.html"/>
<link rel="next" href="modelo-sin-intercepto.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelos no paramétricos y de Regresión</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i>Objetivos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I Introduccion</b></span></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i>Modelos lineales</a>
<ul>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#principios-de-la-modelización-estadística"><i class="fa fa-check"></i>Principios de la modelización estadística</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#identificar-y-caracterizar-variables"><i class="fa fa-check"></i>Identificar y Caracterizar Variables</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#tipos-de-variables-y-tipo-de-modelo"><i class="fa fa-check"></i>Tipos de variables y tipo de modelo</a>
<ul>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-las-variables-explicativas"><i class="fa fa-check"></i>En función de las variables explicativas</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-la-variable-respuesta"><i class="fa fa-check"></i>En función de la variable respuesta</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-modelo-lineal-general"><i class="fa fa-check"></i>El modelo lineal general</a></li>
</ul></li>
<li class="part"><span><b>II Regresión Lineal Simple</b></span></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a>
<ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i>Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i>El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#regresión-lineal-simple"><i class="fa fa-check"></i>Regresión lineal simple</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html"><i class="fa fa-check"></i><b>1</b> Modelo con intercepto</a>
<ul>
<li class="chapter" data-level="1.1" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo"><i class="fa fa-check"></i><b>1.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="1.2" data-path="modelo-con-intercepto.html"><a href="modelo-con-intercepto.html#propiedades-de-los-estimadores"><i class="fa fa-check"></i><b>1.2</b> Propiedades de los estimadores</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html"><i class="fa fa-check"></i><b>2</b> Modelo sin intercepto</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-1"><i class="fa fa-check"></i><b>2.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.2" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#propiedades-de-los-estimadores-1"><i class="fa fa-check"></i><b>2.2</b> Propiedades de los estimadores</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="modelo-sin-intercepto.html"><a href="modelo-sin-intercepto.html#ejemplo-en-r-studio"><i class="fa fa-check"></i><b>2.2.1</b> Ejemplo en R-Studio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html"><i class="fa fa-check"></i><b>3</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="3.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-beta_0"><i class="fa fa-check"></i><b>3.1</b> Intervalo para <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-beta_1"><i class="fa fa-check"></i><b>3.2</b> Intervalo para <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-sigma2"><i class="fa fa-check"></i><b>3.3</b> Intervalo para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-para-el-valor-esperado-y"><i class="fa fa-check"></i><b>3.4</b> Intervalo para el valor esperado <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalo-de-predicción"><i class="fa fa-check"></i><b>3.5</b> Intervalo de predicción</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#ejemplo"><i class="fa fa-check"></i><b>3.5.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html"><i class="fa fa-check"></i><b>4</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-para-beta_0"><i class="fa fa-check"></i><b>4.1</b> Pruebas para <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="4.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#prueba-para-beta_1"><i class="fa fa-check"></i><b>4.2</b> Prueba para <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#prueba-para-sigma2"><i class="fa fa-check"></i><b>4.3</b> Prueba para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#análisis-de-la-varianza-anova"><i class="fa fa-check"></i><b>4.4</b> Análisis de la varianza (ANOVA)</a></li>
<li class="chapter" data-level="4.5" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#coeficiente-de-determinación"><i class="fa fa-check"></i><b>4.5</b> Coeficiente de determinación</a></li>
<li class="chapter" data-level="4.6" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#propiedades-de-r2"><i class="fa fa-check"></i><b>4.6</b> Propiedades de <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="4.7" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#relación-r2-y-la-correlación-de-pearson"><i class="fa fa-check"></i><b>4.7</b> Relación <span class="math inline">\(R^2\)</span> y la correlación de Pearson</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#ejemplo-1"><i class="fa fa-check"></i><b>4.7.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html"><i class="fa fa-check"></i><b>5</b> Validación de supuestos</a>
<ul>
<li class="chapter" data-level="5.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#análisis-de-residuales"><i class="fa fa-check"></i><b>5.1</b> Análisis de residuales</a></li>
<li class="chapter" data-level="5.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-normalidad"><i class="fa fa-check"></i><b>5.2</b> Supuesto de normalidad</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#validación-del-supuesto-de-normalidad"><i class="fa fa-check"></i><b>5.2.1</b> Validación del supuesto de normalidad</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-linealidad"><i class="fa fa-check"></i><b>5.3</b> Supuesto de linealidad</a></li>
<li class="chapter" data-level="5.4" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#supuesto-de-homocedasticidad"><i class="fa fa-check"></i><b>5.4</b> Supuesto de homocedasticidad</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#prueba-de-breusch-pagan"><i class="fa fa-check"></i><b>5.4.1</b> Prueba de Breusch-Pagan</a></li>
<li class="chapter" data-level="5.4.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#prueba-de-white"><i class="fa fa-check"></i><b>5.4.2</b> Prueba de White</a></li>
<li class="chapter" data-level="5.4.3" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#ejemplo-2"><i class="fa fa-check"></i><b>5.4.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-outlier-e-influyentes"><i class="fa fa-check"></i><b>5.5</b> Valores outlier e influyentes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-outlier"><i class="fa fa-check"></i><b>5.5.1</b> Valores outlier</a></li>
<li class="chapter" data-level="5.5.2" data-path="validación-de-supuestos.html"><a href="validación-de-supuestos.html#valores-influentes"><i class="fa fa-check"></i><b>5.5.2</b> Valores influentes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html"><i class="fa fa-check"></i><b>6</b> Ejemplo de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#contexto-del-problema"><i class="fa fa-check"></i><b>6.1</b> Contexto del problema</a></li>
<li class="chapter" data-level="6.2" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#exploración-de-datos"><i class="fa fa-check"></i><b>6.2</b> Exploración de datos</a></li>
<li class="chapter" data-level="6.3" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#visualización-de-datos"><i class="fa fa-check"></i><b>6.3</b> Visualización de datos</a></li>
<li class="chapter" data-level="6.4" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#ajuste-del-modelo-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>6.4</b> Ajuste del modelo de regresión lineal simple</a></li>
<li class="chapter" data-level="6.5" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#validación-del-modelo"><i class="fa fa-check"></i><b>6.5</b> Validación del modelo</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#coefficients"><i class="fa fa-check"></i><b>6.5.1</b> Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>6.6</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="6.7" data-path="ejemplo-de-regresión-lineal-simple.html"><a href="ejemplo-de-regresión-lineal-simple.html#intepretación-de-resultados"><i class="fa fa-check"></i><b>6.7</b> Intepretación de resultados</a></li>
</ul></li>
<li class="part"><span><b>III Modelo de regresión lineal múltiple</b></span></li>
<li class="chapter" data-level="" data-path="introducción-1.html"><a href="introducción-1.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="7" data-path="modelo.html"><a href="modelo.html"><i class="fa fa-check"></i><b>7</b> Modelo</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modelo.html"><a href="modelo.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo-2"><i class="fa fa-check"></i><b>7.1</b> Estimación por mínimos cuadrados de los parámetros del modelo</a></li>
<li class="chapter" data-level="7.2" data-path="modelo.html"><a href="modelo.html#estimación-por-máxima-verosimilitud"><i class="fa fa-check"></i><b>7.2</b> Estimación por máxima verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html"><i class="fa fa-check"></i><b>8</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="8.1" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalo-para-beta_j"><i class="fa fa-check"></i><b>8.1</b> Intervalo para <span class="math inline">\(\beta_{j}\)</span></a></li>
<li class="chapter" data-level="8.2" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalo-para-sigma2-1"><i class="fa fa-check"></i><b>8.2</b> Intervalo para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="8.3" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalos-de-la-respuesta-media"><i class="fa fa-check"></i><b>8.3</b> Intervalos de la respuesta media</a></li>
<li class="chapter" data-level="8.4" data-path="intervalos-de-confianza-1.html"><a href="intervalos-de-confianza-1.html#intervalos-de-predicción"><i class="fa fa-check"></i><b>8.4</b> Intervalos de predicción</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html"><i class="fa fa-check"></i><b>9</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#región-de-rechazo-para-beta_j"><i class="fa fa-check"></i><b>9.1</b> Región de rechazo para <span class="math inline">\(\beta_{j}\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#prueba-para-sigma2-1"><i class="fa fa-check"></i><b>9.2</b> Prueba para <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#análisis-de-la-varianza-anova-1"><i class="fa fa-check"></i><b>9.3</b> Análisis de la varianza (ANOVA)</a></li>
<li class="chapter" data-level="9.4" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#coeficiente-de-determinación-2"><i class="fa fa-check"></i><b>9.4</b> Coeficiente de determinación</a></li>
<li class="chapter" data-level="9.5" data-path="pruebas-de-hipótesis-1.html"><a href="pruebas-de-hipótesis-1.html#r2-ajustado"><i class="fa fa-check"></i><b>9.5</b> <span class="math inline">\(R^2\)</span> ajustado</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html"><i class="fa fa-check"></i><b>10</b> Validación de supuestos</a>
<ul>
<li class="chapter" data-level="10.1" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#supuesto-de-multicolinealidad"><i class="fa fa-check"></i><b>10.1</b> Supuesto de multicolinealidad</a></li>
<li class="chapter" data-level="10.2" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#detección-de-multicolinealidad"><i class="fa fa-check"></i><b>10.2</b> Detección de multicolinealidad</a></li>
<li class="chapter" data-level="10.3" data-path="validación-de-supuestos-1.html"><a href="validación-de-supuestos-1.html#ejemplo-3"><i class="fa fa-check"></i><b>10.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="apéndice.html"><a href="apéndice.html"><i class="fa fa-check"></i><b>11</b> Apéndice</a></li>
<li class="chapter" data-level="12" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html"><i class="fa fa-check"></i><b>12</b> Ejemplo de regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#contexto-del-problema-1"><i class="fa fa-check"></i><b>12.1</b> Contexto del problema</a></li>
<li class="chapter" data-level="12.2" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#exploración-de-datos-1"><i class="fa fa-check"></i><b>12.2</b> Exploración de datos</a></li>
<li class="chapter" data-level="12.3" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#visualización-de-datos-1"><i class="fa fa-check"></i><b>12.3</b> Visualización de datos</a></li>
<li class="chapter" data-level="12.4" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#ajuste-del-modelo-de-regresión-lineal-múltiple"><i class="fa fa-check"></i><b>12.4</b> Ajuste del modelo de regresión lineal múltiple</a></li>
<li class="chapter" data-level="12.5" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#validación-del-modelo-1"><i class="fa fa-check"></i><b>12.5</b> Validación del modelo</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#coefficients-1"><i class="fa fa-check"></i><b>12.5.1</b> Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#bondad-de-ajuste-1"><i class="fa fa-check"></i><b>12.6</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="12.7" data-path="ejemplo-de-regresión-lineal-múltiple.html"><a href="ejemplo-de-regresión-lineal-múltiple.html#intepretación-de-resultados-1"><i class="fa fa-check"></i><b>12.7</b> Intepretación de resultados</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Hecho con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelos de Regresión</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelo-con-intercepto" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Capítulo 1</span> Modelo con intercepto<a href="modelo-con-intercepto.html#modelo-con-intercepto" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>El objetivo principal del <strong>Modelo de Regresión Lineal Simple</strong> es el poder asociar o ajustar a una dispersión de datos una función (en primera instancia se abordará el ajuste mediante una recta) cuyos parámetros dependan directamente de las observaciones con la finalidad de poder resumir, simplificar u obtener propiedades importantes sobre comportamiento de la muestra.</p>
<p>Dicho modelo es el más sencillo de los modelos lineales e involucra una variable de interés <span class="math inline">\(y\)</span> llamada <strong>dependiente</strong> o <strong>respuesta</strong> y su relación con la variable <strong>predictoria</strong> o <strong>independiente</strong> <span class="math inline">\(x\)</span>, estableciendo que la media de la variable dependiente <span class="math inline">\(y\)</span> cambia a razón constante cuando el valor de la variable independiente <span class="math inline">\(x\)</span> crece o decrece.</p>
<p>El modelo de regresión lineal simple en general es:</p>
<p><span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\]</span>
donde:</p>
<ul>
<li><p>x es la variable regresora,</p></li>
<li><p>y es la variable de respuesta,</p></li>
<li><p><span class="math inline">\(\beta_{0}\)</span> ordenada al origen,</p></li>
<li><p><span class="math inline">\(\beta_{1}\)</span> pendiente del modelo,</p></li>
<li><p><span class="math inline">\(\epsilon\)</span> es un error aleatorio.</p></li>
</ul>
<p>Conviene considerar a la variable regresora <span class="math inline">\(x\)</span> como una varible determinista, o bien, una variable controlada por el investigador la cual puede ser medida, mientras que la variable respuesta <span class="math inline">\(y\)</span> es una variable aleatoria. Ahora bien, los datos no caen exactamente sobre una recta por lo que se considera <span class="math inline">\(\epsilon\)</span> como un error estadístico, esto es, que es una variable aleatoria que explica por qué el modelo no ajusta exactamente los datos.</p>
<p>Una vez vista la ecuación general del modelo de regresión lineal simple se hablará de algunos supuestos que se deben de cumplir al ajustar una serie de datos, éstas consideraciones hace que en ocaciones carezca de sentido realizar una regresión lineal, sin embargo, no hay que perder de vista que éste es un modelado por lo que algunas características físicas del problema pueden haber sido simplificadas u omitidas.</p>
<p><strong>Definición 2.1</strong> (Supuestos del Modelo de Regresión Simple).</p>
<p>En el modelo de regresión simple se supone que <span class="math inline">\(\epsilon\)</span> satisface:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{E}[\epsilon_{i}]=0\)</span></p></li>
<li><p><span class="math inline">\(\textbf{Var}(\epsilon_{i})=\sigma^2\)</span></p></li>
<li><p><span class="math inline">\(\textbf{Cov}(\epsilon_{i},\epsilon_{j})= 0 \  \ \ \forall \ i = 1, \ldots, n, \ \  j=1, \ldots, n, \ \  i \neq j.\)</span></p></li>
<li><p><span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span></p></li>
</ul>
<p>Una vez considerados estos supuestos se pueden obtener resultados aún más importantes.</p>
<p>Por ejemplo, con los supuestos dados es posible calcular la esperanza y la varianza de la variable <span class="math inline">\(y_{i}\)</span>, dado un valor <span class="math inline">\(x_{i}\)</span>.</p>
<p><strong>Teorema 2.1</strong> Sea <span class="math inline">\(y\)</span> una variable de interés, denominada variable de respuesta, la cual es relacionada con una variable regresora <span class="math inline">\(x\)</span>, entonces:</p>
<p><strong>a)</strong> <span class="math inline">\(\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(\textbf{Var}(y_{i})=\sigma^2\)</span></p>
<p><strong>Demostración:</strong></p>
<p><strong>a)</strong></p>
<p><span class="math display">\[\mathbf{E}[y_{i}]=\mathbf{E}[\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}]\]</span></p>
<p>La parte aleatoria de <span class="math inline">\(y_{i}\)</span> es <span class="math inline">\(\epsilon_{i}\)</span>;   <span class="math inline">\(\beta_{0}\)</span>,<span class="math inline">\(\beta_{1}\)</span> son constantes y <span class="math inline">\(x_{i}\)</span> ya es un valor dado; por lo que:</p>
<p><span class="math display">\[\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}+\mathbf{E}[\epsilon_{i}]\]</span></p>
<p><span class="math display">\[=\beta_{0}+\beta_{1}x_{i}+0\]</span>
<span class="math display">\[\therefore \mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}.\]</span>
<strong>b)</strong></p>
<p><span class="math display">\[Var(y_{i})= Var(\beta_{0}+\beta_{1}x_{i}+\epsilon{i})\]</span>
La parte aleatoria de <span class="math inline">\(y_{i}\)</span> es <span class="math inline">\(\epsilon_{i}\)</span>; <span class="math inline">\(\beta_{0}\)</span>,<span class="math inline">\(\beta_{1}\)</span> son constantes y <span class="math inline">\(x_{i}\)</span> ya es un valor dado; por lo que <span class="math inline">\(Var(c+\epsilon_{i})=Var(\epsilon_{i})\)</span> con <span class="math inline">\(c\)</span> constante, de esta manera:</p>
<p><span class="math display">\[Var(y_{i})=0+0+Var(\epsilon_{i})\]</span>
<span class="math display">\[\therefore Var(y_{i})=\sigma^2\]</span></p>
<div id="estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Estimación por mínimos cuadrados de los parámetros del modelo<a href="modelo-con-intercepto.html#estimación-por-mínimos-cuadrados-de-los-parámetros-del-modelo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El modelo de regresión lineal simple</p>
<p><span class="math display">\[y=\beta_{0}+\beta_{1}x+\epsilon\]</span></p>
<p>cuenta con dos parámetros desconocidos, <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span>, los cuales deben ser estimados a partir de los datos de la muestra. Con la hipótesis de varianza constante sobre los errores, aparece otro parámetro <span class="math inline">\(\sigma^2\)</span> desconocido, aunque no está incluido en el modelo también debe ser estimado.
Un procedimiento para estimar los parámetros de un modelo lineal simple es el <strong>método de mínimos cuadrados</strong>, que se puede ilustar sencillamente aplicándolo para ajustar una línea recta a <span class="math inline">\((x_{1},y_{1}),(x_{2},y_{2}),\ldots,(x_{n},y_{n})\)</span>, se estiman <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> tales que la suma de los cuadrados de las diferencias entre las observaciones <span class="math inline">\(y_{i}\)</span> y la línea recta sea mínima.</p>
<p><strong>Definición 2.2</strong> (Residuos). Sea <span class="math inline">\(y_{i}\)</span> los valores observados, <span class="math inline">\(\hat{y_{i}}\)</span> los valores estimados mediante la regresión lineal simple. La forma de calcular la desviación de <span class="math inline">\(y_{i}\)</span> con respecto a su media estimada <span class="math inline">\(\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\)</span> para un <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> dados es:</p>
<p><span class="math display">\[e_{i}=y_{i}-\hat{y_{i}}\]</span>
donde <span class="math inline">\(e_{i}\)</span> son los residuos.</p>
<p>Por lo anterior,</p>
<p><span class="math display">\[e_{i}=y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i}\]</span>
Como se mencionó, lo que se busca es que la diferencia entre todos los valores observados y los valores estimados sea 0, es decir, que la suma de distancias entre <span class="math inline">\(y_{i}\)</span> y <span class="math inline">\(\hat{y_{i}}\)</span> sea cero, lo anterior significa que:</p>
<p><span class="math display">\[\sum_{i=1}^{n}e_{i}=0\]</span>
Para estimar <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> el <strong>método de mínimos cuadrados</strong> propone minimizar la suma de los cuadrados de los residuos, ya que de ésta manera se minimizan las distancias verticales entre las observaciones reales <span class="math inline">\((y)\)</span> y las estimadas <span class="math inline">\((\hat{y})\)</span>, ya que entre más cercanas a cero se encuentren las distancias, mejor se ajusta el modelo a los datos.
Antes de continuar, es necesario abordar unos cuantos resultados.</p>
<p>Se define a:</p>
<p><span class="math display">\[S_{xx}=\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}(x_{i}^2-2x_{i}\overline{x}+\overline{x}_{i}^{2})\]</span>
<span class="math display">\[=\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}2x_{i}\overline{x}+\sum_{i=1}^{n}\overline{x}_{i}^2\]</span>
<span class="math display">\[=\sum_{i=1}^{n}x_{i}^2-2\overline{x}^2n+\overline{x}_{i}^2n\]</span></p>
<p><span class="math display">\[S_{xx}=\sum_{i=1}^{n}x_{i}^2-\overline{x}^2n.\]</span></p>
<p>También definimos:</p>
<p><span class="math display">\[S_{xy}=\sum_{i=1}^{n}(xi-\overline{x})(y_{i}-\overline{y})\]</span></p>
<p><span class="math display">\[=\sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}\]</span></p>
<p><span class="math display">\[=\sum_{i=1}^{n}x_{i}(y_{i}-\overline{y})\]</span>
<span class="math display">\[=\sum_{i=1}^{n}X_{i}y_{i}-\left(\sum_{i=1}^{n}x_{i}\right) \left(\sum_{i=1}^{n}y_{i}\right) \ /{n}\]</span></p>
<p><span class="math display">\[S_{xy}=\sum_{i=1}^{n}x_{i}y_{i}-n\overline{x}\overline{y}\]</span></p>
<p>Una vez definida la notación, tenemos el siguiente teorema:</p>
<p><strong>Teorema 2.2</strong> (Mínimos Cuadrados). Sea <span class="math inline">\(\hat{\beta_0}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> los parámetros que minimizan la suma de cuadrados de la diferencia entre los valores observados y los estimados <span class="math inline">\(\left(\sum_{i=1}^{n}e_i^2\right)\)</span> entonces:</p>
<p><strong>a)</strong> <span class="math inline">\(\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(\hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\)</span></p>
<p><strong>Demostración:</strong></p>
<p><strong>a)</strong></p>
<p>Tenemos:</p>
<p><span class="math display">\[\sum_{i=1}^{n}e_{i}^2=\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})^2.\]</span>
Minimizando la suma de cuadrados, se deriva respecto a <span class="math inline">\(\hat{\beta_{0}}\)</span></p>
<p><span class="math display">\[\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{0}}}=-2\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})\]</span>
<span class="math display">\[\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{0}}}=-2\left(\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}-\sum_{i=1}^{n}\hat{\beta_{1}x_{i}}\right).\]</span>
Igualando a 0</p>
<p><span class="math display">\[-2\left(\sum_{i=1}^{n}y_{i}-\hat{\beta_{0}}n-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}\right)=0\]</span>
<span class="math display">\[\sum_{i=1}^{n}y_{i}-\hat{\beta_{0}}n-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}=0\]</span>
<span class="math display">\[\sum_{i=1}^{n}y_{i}=n\hat{\beta_{0}}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}\]</span>
<span class="math display">\[\overline{y}n=n\hat{\beta_{0}}+\hat{\beta_{1}}\overline{x}n\]</span>
<span class="math display">\[\overline{y}=\hat{\beta_{0}}+\hat{\beta_{1}}\overline{x}\]</span>
<span class="math display">\[\therefore\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}.\blacksquare\]</span>
Por lo tanto se obtiene el primer estimador, <span class="math inline">\(\hat{\beta_{0}}\)</span>; para el estimador de <span class="math inline">\(\beta_{1}\)</span> se deriva respecto a <span class="math inline">\(\hat{\beta_{1}}\)</span>:</p>
<p><span class="math display">\[\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{1}}}=-2\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})x_{i}\]</span>
<span class="math display">\[=-2\left(\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2\right).\]</span>
Igualando la derivada a 0 para hallar el punto crítico</p>
<p><span class="math display">\[-2\left(\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2\right)=0\]</span></p>
<p><span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2=0\]</span>
<span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}=\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}+\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2\]</span>
<span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}=\hat{\beta_{0}}\sum_{i=1}^{n}x_{i}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2\]</span>
<span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}=\hat{\beta_{0}}\overline{x}n+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2\]</span>
Sustituyendo <span class="math inline">\(\hat{\beta_{0}}\)</span> por <span class="math inline">\(\overline{y}-\hat{\beta_{1}}\overline{x}\)</span> se tiene:</p>
<p><span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}=\overline{y}\overline{x}n-\hat{\beta_{1}}\overline{x}^2n+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2\]</span>
<span class="math display">\[\sum_{i=1}^{n}x_{i}y_{i}-\overline{y}\overline{x}n+\hat{\beta_{1}}\overline{x}^2n-\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2\]</span>
Por la notación tenemos que: <span class="math inline">\(S_{xy}=\sum x_{i}y_{i} - n \overline{x}\overline{y}\)</span> y <span class="math inline">\(S_{xx}= \sum x_{i}^2 - n\overline{x}^2\)</span></p>
<p><span class="math display">\[\frac{\delta \sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{1}}}=-2\left(S_{xy}-\hat{\beta_{1}\left(\sum_{i=1}^{n}x_{i}^2-\overline{x}^2n\right)}\right)\]</span>
<span class="math display">\[=-2(S_{xy}-\hat{\beta_{1}}S_{xx}).\]</span>
Igualando a 0</p>
<p><span class="math display">\[-2(S_{xy}-\hat{\beta_{1}}S_{xx})=0\]</span>
<span class="math display">\[S_{xy}-\hat{\beta_{1}}S_{xx}=0\]</span>
<span class="math display">\[\Rightarrow \hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}\]</span>
<span class="math display">\[\therefore \hat{\beta_{1}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}.\blacksquare\]</span>
De esta manera se demuestra el teorema 2.2. Un punto a destacar es que las siguientes ecuaciones:</p>
<p><span class="math display">\[
\left\{
\begin{array}{ll} \hat{\beta_{0}} \ \ n  \ \ \  \ \ \ \ \ \ + \ \hat{\beta_{1}}\sum_{i=1}^{n}x_{i}\  \ = \ \sum_{i=1}^{n}y_{i} \\
\\
\hat{\beta_{0}} \ \sum_{i=1}^{n}x_{i} \ + \ \hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2 \ \ = \ \sum_{i=1}^{n}x_{i}y_{i}
\end{array}
\right.
\]</span></p>
<p>Son conocidas como las <strong>ecuaciones normales</strong>, que en conjunto forman un sistema de ecuaciones; al resolverlas simultáneamente para <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> se obtiene los estimadores de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> que se plantean en el teorema anterior.</p>
<p>Un problema del método de mínimos cuadrados es que no proporciona un estimador para <span class="math inline">\(\sigma^2\)</span>, sin embargo, se obtendrá bajo el supuesto de normalidad que es el siguiente:</p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^2\]</span>
De esta manera se ha cumplido el objetivo que se planteó al inicio, encontrar los estimadores de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> tal que los residuales fueran igual a 0.</p>
<p><strong>Teorema 2.3</strong> (Diferencia de Residuales) Sea <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> los estimadores de mínimos cuadrados de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span>, respectivamente, entonces la suma de las distancias entre <span class="math inline">\(y_{i}\)</span> y <span class="math inline">\(\hat{y_{i}}\)</span> es cero, es decir:</p>
<p><span class="math display">\[\sum_{i=1}^{n}e_{i}=0\]</span></p>
<p><strong>Demostración</strong></p>
<p>Tenemos que el residual <span class="math inline">\(e_{i}\)</span> se calcula como: <span class="math inline">\(e_{i}=y_{i}-\hat{\beta_{0}}-\hat{\beta_1}x_{i}\)</span> entonces si sustituimos se tiene:</p>
<p><span class="math display">\[\sum_{i=1}^{n}e_{i}=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}-\hat{\beta_1}x_{i}\right)\]</span></p>
<p><span class="math display">\[=\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}\]</span>
<span class="math display">\[=n\overline{y}-n\hat{\beta_{0}}-n\hat{\beta_{1}}\overline{x}\]</span>
<span class="math display">\[=n\left[\overline{y}-\hat{\beta_{0}}-\hat{\beta_{1}}\overline{x}\right]\]</span>
Recordando que la estimación de <span class="math inline">\(\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}\)</span> tenemos:</p>
<p><span class="math display">\[=n\left[\overline{y}-\overline{y}+\hat{\beta_{1}}\overline{x}-\hat{\beta_{1}}\overline{x}\right]\]</span></p>
<p><span class="math inline">\(\therefore \sum_{i=1}^{n}e_{i}=0. \blacksquare\)</span></p>
<p>Esto implica que si la suma de residuales es 0, entonces la <span class="math inline">\(\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}\)</span> y vamos a demostrarlo:</p>
<p><strong>Corolario 1</strong> Sea <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> los estimadores de mínimos cuadrados de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> respectivamente, si se cumple el teorema anterior, entonces la suma de los valores observados y la suma de los valores ajustados por la regresión son iguales, es decir:</p>
<p><span class="math display">\[\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}\]</span></p>
<p><strong>Demostración</strong></p>
<p>Por definición:</p>
<p><span class="math display">\[\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}\left(\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\right)\]</span>
Por la estimación de <span class="math inline">\(\hat{\beta_{1}}\)</span></p>
<p><span class="math display">\[=\sum_{i=1}^{n}\left(\overline{y}-\hat{\beta_{1}}\overline{x}+\hat{\beta_{1}}x_{i}\right)\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\overline{y}-n\hat{\beta_{1}}\overline{x}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}\]</span>
<span class="math display">\[=n\overline{y}-n\hat{\beta_{1}}\overline{x}+n\hat{\beta_{1}}\overline{x}\]</span>
<span class="math display">\[=n\overline{y}\]</span>
Por construcción de <span class="math inline">\(\overline{y}\)</span></p>
<p><span class="math display">\[\therefore \sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}. \blacksquare\]</span></p>
<p>Una consecuencia de lo anterior,</p>
<p><strong>Corolario 2</strong> Sea <span class="math inline">\(\hat{y_{i}}\)</span> el valor estimado de la forma   <span class="math inline">\(\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\)</span>   y el residual  <span class="math inline">\(e_{i}=y_{i}-\hat{y_{i}}\)</span>, entonces se cumple que:</p>
<p><span class="math display">\[\sum_{i=1}^{n}\hat{y_{i}}e_{i}=0\]</span></p>
<p><strong>Demostración</strong></p>
<p>Por definición de las <span class="math inline">\(\hat{y}\)</span></p>
<p><span class="math display">\[\sum_{i=1}^{n}\hat{y_{i}}e_{i}=\sum_{i=1}^{n}(\hat{\beta_{0}}+\hat{\beta_{1}}x_{i})e_{i}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\hat{\beta_{0}}e_{i}+\hat{\beta_{1}}x_{i}e_{i}\right)\]</span></p>
<p><span class="math display">\[=\sum_{i=1}^{n}\hat{\beta_{0}}e_{i}+\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}e_{i}\]</span>
<span class="math display">\[=\hat{\beta_{0}}\sum_{i=1}^{n}e_{i}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}e_{i}\]</span>
Y como demostramos anteriormente, la suma de los residuos es cero:</p>
<p><span class="math display">\[=\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}(y_{i}-\hat{y_{i}})\]</span>
<span class="math display">\[=\hat{\beta_{1}}\sum_{i=1}^{n}\left(x_{i}y_{i}-x_{i}\hat{\beta_{0}}-x_{i}^{2}\hat{\beta_{1}}\right)\]</span>
<span class="math display">\[=\hat{\beta_{1}}\sum_{i=1}^{n}\left(x_{i}y_{i}-x_{i}^2\hat{\beta_{1}}-x_{i}y_{i}+x_{i}^{2}\hat{\beta_{1}}\right)\]</span></p>
<p>Entonces:</p>
<p><span class="math display">\[\sum_{i=1}^{n}\hat{y_{i}}e_{i}=0. \blacksquare\]</span></p>
</div>
<div id="propiedades-de-los-estimadores" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Propiedades de los estimadores<a href="modelo-con-intercepto.html#propiedades-de-los-estimadores" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Los estimadores <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span> tienen propiedades estadísticas muy importantes. ya que son estimadores insesgados, además son de mínima varianza. La propiedad de insesgamiento se revisará en el siguiente teorema:</p>
<p><strong>Teorema 2.4</strong> Sea <span class="math inline">\(\hat{\beta_{0}}\)</span>, <span class="math inline">\(\hat{\beta_{1}}\)</span> los estimadores de mínimos cuadrados de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span>, respectivamente, entonces los estimadores <span class="math inline">\(\hat{\beta_{0}}\)</span>,<span class="math inline">\(\hat{\beta_{1}}\)</span> son insesgados. Es decir:</p>
<p><strong>a)</strong> <span class="math inline">\(\mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{1}\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(\mathbf{E}\left[\hat{\beta_{0}}\right]=\beta_{0}\)</span></p>
<p><strong>Demostración</strong></p>
<p><strong>a)</strong> Para demostrar el insesgamiento de <span class="math inline">\(\hat{\beta_{1}}\)</span> usaremos la propiedad de combinación lineal de <span class="math inline">\(\beta_{1}:\)</span></p>
<p>“Haremos un pequeño paréntesis para demostrarlo”</p>
<p>Tenemos:</p>
<p><span class="math display">\[\hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}\]</span>
Sustituyendo <span class="math inline">\(S_{xx}\)</span> y <span class="math inline">\(S_{xy}\)</span></p>
<p><span class="math display">\[\hat{\beta_{1}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}}{S_{xx}}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\frac{x_{i}-\overline{x}}{S_{xx}}\right)y_{i}.\]</span>
Haciendo <span class="math inline">\(a=\frac{(x_{i}-\overline{x})}{S_{xx}}\)</span> nada depende de <span class="math inline">\(y\)</span>, por lo que <span class="math inline">\(a\)</span> es constante, por consiguiente la combinación lineal de las <span class="math inline">\(y_{i}\)</span> para <span class="math inline">\(\hat{\beta_{1}}\)</span> es:</p>
<p><span class="math display">\[\therefore \hat{\beta_{1}}=\sum_{i=1}^{n}ay_{i}.\]</span>
Para la combinación lineal de las <span class="math inline">\(y_{i}\)</span> para <span class="math inline">\(\hat{\beta_{0}}\)</span> se tiene:</p>
<p><span class="math display">\[\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}\]</span>
Sustituyendo la combinación lineal de <span class="math inline">\(\hat{\beta_{1}}\)</span> y notación de <span class="math inline">\(S_{xx}\)</span></p>
<p><span class="math display">\[\hat{\beta_{0}}=\sum_{i=1}^{n}\frac{y_{i}}{n}-ay_{i}\overline{x}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}(\frac{1}{n}-a\overline{x})y_{i}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\right)y_{i}. \]</span>
Haciendo <span class="math inline">\(b=\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\)</span> nada depende de <span class="math inline">\(y\)</span>, por lo que <span class="math inline">\(b\)</span> es constante, entonces la combinación lineal de <span class="math inline">\(y_{i}\)</span> para <span class="math inline">\(\hat{\beta_{0}}\)</span> es:</p>
<p><span class="math display">\[\therefore \hat{\beta_{0}}=\sum_{i=1}^{n}by_{i}.\]</span></p>
<p>“Regresando del paréntesis tenemos:”</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{1}}\right]=\mathbf{E}\left[\sum_{i=1}^{n}ay_{i}\right]\]</span>
<span class="math display">\[=\sum_{i=1}^{n}a\mathbf{E}[y_{i}].\]</span>
Tenemos que <span class="math inline">\(\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}\)</span>, sustituyendo:</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{1}}\right]=\sum_{i=1}^{n}a(\beta_{0}+\beta_{1}x_{i})\]</span>
<span class="math display">\[=\sum_{i=1}^{n}a\beta_{0}+\sum_{i=1}^{n}a\beta_{1}x_{i}\]</span>
<span class="math display">\[=\beta_{0}\sum_{i=1}^{n}a+\beta_{1}\sum_{i=1}^{n}ax_{i}.\]</span>
Sustituyendo <span class="math inline">\(a\)</span> de la linealidad de <span class="math inline">\(\beta_{1}\)</span> se tiene:</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{0}\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)+\beta_{1}\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)x_{i}\]</span>
<span class="math display">\[=\frac{\beta_{0}}{S_{xx}}(n\overline{x}-\overline{x}n)+\frac{\beta_{1}}{S_{xx}}\sum_{i=1}^{n}(x_{i}-\overline{x})x_{i}.\]</span></p>
<p>Simplificando y recordando que <span class="math inline">\(\sum_{i=1}^{n}(x_{i}-\overline{x})x_{i}=S_{xx}\)</span></p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{1}}\right]=0+\frac{\beta_{1}}{S_{xx}}S_{xx}\]</span></p>
<p><span class="math display">\[\therefore \mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{1}\]</span></p>
<p>Por lo tanto el estimador <span class="math inline">\(\hat{\beta_{1}}\)</span> es insesgado. <span class="math inline">\(\blacksquare\)</span></p>
<p><strong>b)</strong> Ahora para demostrar el insesgamiento de <span class="math inline">\(\hat{\beta_{0}}\)</span>, se sustituye el estimador <span class="math inline">\(\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}\)</span>, de esta forma tenemos:</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{0}}\right]=\mathbf{E}\left[\overline{y}-\hat{\beta_{1}}\overline{x}\right]\]</span>
<span class="math display">\[=\mathbf{E}\left[\overline{y}\right]-\mathbf{E}\left[\hat{\beta_{1}}\overline{x}\right]\]</span>
<span class="math display">\[=\mathbf{E}\left[\sum_{i=1}^{n}\frac{y_{i}}{n}\right]-\overline{x}\mathbf{E}\left[\hat{\beta_{1}}\right]\]</span>
Como el estimador de <span class="math inline">\(\beta_{1}\)</span> es insesgado:</p>
<p><span class="math display">\[=\sum_{i=1}^{n}\frac{1}{n}\mathbf{E}\left(y\right)-\overline{x}\beta_{1}.\]</span></p>
<p>y sabemos que <span class="math inline">\(\mathbf{E}(y)=\beta_{0}+\beta_{1}x,\)</span> sustituyendo tenemos:</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{\beta_{0}}\right]=\sum_{i=1}^{n}\frac{1}{n}(\beta_{0}+\beta_{1}x_{i})-\overline{x}\beta_{1}\]</span>
<span class="math display">\[=\beta_{0}+\beta_{1}\overline{x}-\overline{x}\beta_{1}\]</span>
<span class="math display">\[\therefore \mathbf{E}[\hat{\beta_{0}}]=\beta_{0}.\]</span>
Por lo tanto el estimador <span class="math inline">\(\hat{\beta_{0}}\)</span> es insesgado. <span class="math inline">\(\blacksquare\)</span></p>
<p>Ahora nos preguntaremos por la varianza de los estimadores, analizando qué tan distante está el estimador del parámetro buscado. Es decir, la varianza es el margen de error que obtiene en la estimación de los parámetros.</p>
<p><strong>Teorema 2.5</strong> Sea <span class="math inline">\(\hat{\beta_{0}}\)</span> <span class="math inline">\(\hat{\beta_{1}}\)</span> los estimadores puntuales de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> entonces la varianza de estimación</p>
<p><strong>a)</strong> <span class="math inline">\(Var\left(\hat{\beta_{0}}\right)=\left(\frac{1}{n}+\frac{\overline{x}^2}{S_{xx}}\right) \sigma^2.\)</span></p>
<p><strong>b)</strong> <span class="math inline">\(Var\left(\hat{\beta_{1}}\right)= \frac{1}{S_{xx}} \sigma^2.\)</span></p>
<p><strong>Demostración:</strong></p>
<p><strong>a)</strong></p>
<p><span class="math display">\[Var\left(\hat{\beta_{0}}\right)=Var\left(\overline{y}-\hat{\beta_{1}}\overline{x}\right)\]</span>
<span class="math display">\[=Var(\overline{y})+Var(\hat{\beta_{1}}\overline{x})-2Cov(\overline{y},\hat{\beta_{1}}\overline{x})\]</span>
<span class="math display">\[=Var(\overline{y})+Var(\hat{\beta_{1}}\overline{x})\]</span>
<span class="math display">\[=Var\left(\sum_{i=1}^{n}\frac{y_{i}}{n} \right)+\overline{x}^2Var(\hat{\beta_{1}})\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\frac{1}{n^2}Var(y_{i})+\overline{x}^2\frac{\sigma^2}{S_{xx}}\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\frac{1}{n^2}\sigma^2+\overline{x}^2\frac{\sigma^2}{S_{xx}}\]</span>
<span class="math display">\[\therefore Var\left(\hat{\beta_{0}}\right)=\left(\frac{1}{n}+\frac{\overline{x}^2}{S_{xx}}\right) \sigma^2. \blacksquare\]</span></p>
<p><strong>b)</strong></p>
<p><span class="math display">\[Var\left(\hat{\beta_{1}}\right)= Var\left( \frac{S_{xy}}{S_{xx}^2}\right)\]</span>
<span class="math display">\[= \frac{1}{S_{xx}}Var\left(S_{xy}\right)\]</span>
<span class="math display">\[=\frac{1}{S_{xx}^2}Var\left( \sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}\right)\]</span>
<span class="math display">\[=\frac{1}{S_{xx}^2} \sum_{i=1}^{n}(x_{i}-\overline{x})^2Var(y_{i})\]</span>
<span class="math display">\[=\frac{1}{S_{xx}^2}\sum_{i=1}^{n}(x_{i}-\overline{x})^2Var(e_{i})\]</span></p>
<p><span class="math display">\[=\frac{1}{S_{xx}^2}S_{xx}\sigma^2\]</span></p>
<p><span class="math display">\[\therefore Var\left(\hat{\beta_{1}}\right)= \frac{1}{S_{xx}} \sigma^2. \blacksquare\]</span></p>
<p>Una característica importante de los estimadores obtenidos es que existe una covarianza conjunta entre <span class="math inline">\(\hat{\beta_{0}}\)</span> y <span class="math inline">\(\hat{\beta_{1}}\)</span>, que veremos a continuación:</p>
<p><strong>Teorema 2.6</strong> Sea <span class="math inline">\(\beta_{0}\)</span>,<span class="math inline">\(\beta_{1}\)</span> los estimadores puntuales de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> entonces la covarianza conjunta de los estimadores es:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sigma^2\left(-\frac{\overline{x}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\right)\]</span></p>
<p><strong>Demostración:</strong></p>
<p>Como mencionamos anteriormente, <span class="math inline">\(\hat{\beta_{0}},\hat{\beta_{1}}\)</span> pueden ser expresadas como combinaciones lineales de <span class="math inline">\(y\)</span> por lo que:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}} \right)= \mathbf{E}\left[ \left( \hat{\beta_{0}}-\mathbf{E}(\hat{\beta_{0}})\right)\left(\hat{\beta_{1}}-\mathbf{E}(\hat{\beta_{1})} \right) \right]\]</span>
<span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}} \right)= \mathbf{E}\left[\left(\sum_{i=1}^{n}a_{i}Y_{i}-\mathbf{E}(\sum_{i=1}^{n}a_{i}Y_{i}) \right)\left(\sum_{i=1}^{n}b_{i}Y_{i}-\mathbf{E}(\sum_{i=1}^{n}b_{i}Y_{i}) \right)\right].\]</span></p>
<p>Por linealidad de la esperanza:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}\mathbf{E}[(Y_{i}-\mathbf{E}(Y_{i}))(Y_{i}-\mathbf{E}(Y_{i}))].\]</span>
Por propiedades de la esperanza:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}Var[Y_{i}].\]</span>
Por el <strong>teorema 2.1</strong> se sabe que la varianza del modelo es:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}\sigma^2\]</span>
Sustituyendo <span class="math inline">\(a=\frac{(x_{i}-\overline{x})}{S_{xx}}\)</span> y <span class="math inline">\(b=\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\)</span> se tiene:</p>
<p><span class="math display">\[Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\left[\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\right]\right)\sigma^2\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{nS_{xx}}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)^2\overline{x}\right)\sigma^2\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{nS_{xx}}-\left(\frac{\overline{x}(x_{i}-\overline{x})}{S^2_{xx}}\right)^2\overline{x}\right)\sigma^2\]</span>
<span class="math display">\[=\sum_{i=1}^{n}\left(\frac{x_{i}-\overline{x}}{nS_{xx}}-\frac{\overline{x}(x_{i}-\overline{x})^2}{S^2_{xx}}\right)\sigma^2\]</span>
<span class="math display">\[=\left(\frac{\overline{x}n -n\overline{x}}{nS_{xx}}-\frac{\overline{x}S_{xx}}{S^2_{xx}}\right)\sigma^2\]</span>
<span class="math display">\[=\left(-\frac{\overline{x}}{S_{xx}}\right)\sigma^2\]</span></p>
<p><span class="math display">\[\therefore Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sigma^2\left(-\frac{\overline{x}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\right).\blacksquare\]</span></p>
<p>Una consecuencia inmediata del <strong>teorema 2.3</strong> y <strong>teorema 2.4</strong>, es que si la suma de residuales es 0, implica que la esperanza del valor observado sea igual a la esperanza del valor estimado, es decir:</p>
<p><strong>Corolario 3</strong> Sea <span class="math inline">\(\hat{\beta_{0}},\hat{\beta_{1}}\)</span> los estimadores de mínimos cuadrados de <span class="math inline">\(\beta_{0}\)</span> y <span class="math inline">\(\beta_{1}\)</span> respectivamente, si se cumple el teorema 2.3, entonces la esperanza de los valores observados y la esperanza de los valores ajustados por la regresión son iguales, es decir:</p>
<p><span class="math display">\[\mathbf{E}\left[\hat{y_{i}}\right]=\mathbf{E}\left[ y_{i} \right]\]</span>
De esta manera lo que se debe demostrar es que se cumple la siguiente igualdad:</p>
<p><span class="math inline">\(\mathbf{Pd.} \ \ \mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=0\)</span></p>
<p><strong>Demostración:</strong></p>
<p><span class="math inline">\(\mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=\mathbf{E}\left[ Y_{i}\right]-\mathbf{E}\left[\hat{y_{i}}\right]\)</span></p>
<p><span class="math inline">\(=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[\hat{y_{i}}\right]\)</span>            Por el teorema 2.1</p>
<p><span class="math inline">\(=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[ \hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\right]\)</span></p>
<p><span class="math inline">\(=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[ \hat{\beta_{0}}\right]-\mathbf{E}\left[\hat{\beta_{1}}x_{i}\right]\)</span></p>
<p><span class="math inline">\(=\beta_{0}+\beta_{1}x_{i}-\beta_{0}-\beta_{1}x_{i}\)</span>       Por el teorema 2.5 son insesgados</p>
<p><span class="math inline">\(\therefore \ \ \mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=0.\  \blacksquare\)</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introducción.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modelo-sin-intercepto.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "github", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
