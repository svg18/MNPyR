# (PART) Regresión Lineal Simple {-}

# Introducción {-}

Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía.
Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de _regresión_.

El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística.

El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, $y$, llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, $x_1, x_2, ..., x_p$. Si $p = 1$, se trata de un modelo de regresión simple y si $p > 1$, de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, $y$, es aleatoria y las variables explicativas son fijas, es decir, no aleatorias.

La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición.

## Objetivos del análisis de regresión {-}

Existen varios objetivos dentro del análisis de regresión, entre otros:

- Determinar el efecto, o relación, entre las variables explicativas y la respuesta.
- Predicción de una observación futura.
- Describir de manera general la estructura de los datos.

## El algorítmo de regresión lineal {-}

Sea $\Phi: \mathcal{X} \rightarrow \mathbb{R}^N$ y consideremos la familia de hipótesis lineales $$H=\{x\mapsto w \cdot \Phi(x)+b | w\in\mathbb{R}^N, b\in\mathbb{R}\}$$

La regresión lineal consiste en buscar la hipótesis $h\in H$ con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: $$\min \frac{1}{m}\sum_{i=1}^{m}(h(x_i)-y_i)^2$$

## Regresión lineal simple {-}

```{r, message=FALSE, warning=FALSE, echo=FALSE}
require(ggplot2)

x <- c(609, 629, 620, 564, 645, 493, 606, 660, 630, 672, 778, 616, 727, 810, 778, 823, 755, 710, 701, 803, 855, 838, 830, 864, 635, 565, 562, 580, 596, 597, 636, 559, 615, 740, 677, 675, 629, 692, 710, 730, 763, 686, 717, 737, 816)

y <- c(241, 222, 233, 207, 247, 189, 226, 240, 215, 231, 263, 220, 271, 284, 279, 272, 268, 278, 238, 255, 308, 281, 288, 306, 236, 204, 216, 225, 220, 219, 201, 213, 228, 234, 237, 217, 211, 238, 221, 281, 292, 251, 231, 275, 275)

ggplot(data.frame(x = x, y = y))+
  geom_point(aes(x, y), colour = "#f6583a", size = 6)+
  geom_smooth(aes(x, y), method = "lm", se = FALSE, colour = "#85037e", size = 2)+
  theme_minimal()+
  labs(y = "y", x = bquote(Phi(x)))
```

Para este modelo supondremos que nuestra respuesta, $y$, es explicada únicamente por una covariable, $x$. 

Entonces, escribimos nuestro modelo como:

$$y^{(i)}=\beta_0+\beta_1x^{(i)}+\epsilon^{(i)},\ \ i=1,2,\dots,n$$
Como podemos observar, se ha propuesto una relación lineal entre la variable $y$ y la variable explicativa $x$, que es nuestro primer supuesto sobre el modelo: La relación funcional entre $x$ y $y$ es una línea recta.

Observamos que la relación no es perfecta, ya que se agrega el término de error, $\epsilon$. Dado que la parte aleatoria del modelo es la variable $y$, asumimos que al error se le “cargan” los errores de medición de $y$, así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. 
Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término:

- $\mathbb{E}(\epsilon^{(i)})=0$
- $\mathbb{V}ar(\epsilon^{(i)})=\sigma^2$
- $\mathbb{C}ov(\epsilon^{(i)},\epsilon^{(j)})=0, \ \forall i\neq j$

NOTA: Los errores $\epsilon^{(i)}$ son variables aleatorias no observables.

# Modelo con intercepto

El objetivo principal del **Modelo de Regresión Lineal Simple** es el poder asociar o ajustar a una dispersión de datos una función (en primera instancia se abordará el ajuste mediante una recta) cuyos parámetros dependan directamente de las observaciones con la finalidad de poder resumir, simplificar u obtener propiedades importantes sobre comportamiento de la muestra.     

Dicho modelo es el más sencillo de los modelos lineales e involucra una variable de interés $y$ llamada **dependiente** o **respuesta** y su relación con la variable **predictoria** o **independiente** $x$, estableciendo que la media de la variable dependiente $y$ cambia a razón constante cuando el valor de la variable independiente $x$ crece o decrece.

El modelo de regresión lineal simple en general es:

$$y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},$$
donde:

* x es la variable regresora,

* y es la variable de respuesta,

* $\beta_{0}$ ordenada al origen,

* $\beta_{1}$ pendiente del modelo,

* $\epsilon$ es un error aleatorio.

Conviene considerar a la variable regresora $x$ como una varible determinista, o bien, una variable controlada por el investigador la cual puede ser medida, mientras que la variable respuesta $y$ es una variable aleatoria. Ahora bien, los datos no caen exactamente sobre una recta por lo que se considera $\epsilon$ como un error estadístico, esto es, que es una variable aleatoria que explica por qué el modelo no ajusta exactamente los datos. 

Una vez vista la ecuación general del modelo de regresión lineal simple se hablará de algunos supuestos que se deben de cumplir al ajustar una serie de datos, éstas consideraciones hace que en ocaciones carezca de sentido realizar una regresión lineal, sin embargo, no hay que perder de vista que éste es un modelado por lo que algunas características físicas del problema pueden haber sido simplificadas u omitidas.

**Definición 2.1** (Supuestos del Modelo de Regresión Simple).

En el modelo de regresión simple se supone que $\epsilon$ satisface:


* $\mathbf{E}[\epsilon_{i}]=0$

* $\textbf{Var}(\epsilon_{i})=\sigma^2$

* $\textbf{Cov}(\epsilon_{i},\epsilon_{j})= 0 \  \ \ \forall \ i = 1, \ldots, n, \ \  j=1, \ldots, n, \ \  i \neq j.$

* $\epsilon \sim N(0,\sigma^2)$

Una vez considerados estos supuestos se pueden obtener resultados aún más importantes.

Por ejemplo, con los supuestos dados es posible calcular la esperanza y la varianza de la variable $y_{i}$, dado un valor $x_{i}$.

**Teorema 2.1** Sea $y$ una variable de interés, denominada variable de respuesta, la cual es relacionada con una variable regresora $x$, entonces:


**a)** $\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}$

**b)**  $\textbf{Var}(y_{i})=\sigma^2$

**Demostración:**

**a)**

$$\mathbf{E}[y_{i}]=\mathbf{E}[\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}]$$


La parte aleatoria de $y_{i}$ es $\epsilon_{i}$; \  $\beta_{0}$,$\beta_{1}$ son constantes y $x_{i}$ ya es un valor dado; por lo que:  

$$\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}+\mathbf{E}[\epsilon_{i}]$$

$$=\beta_{0}+\beta_{1}x_{i}+0$$
$$\therefore \mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}.$$
**b)** 


$$Var(y_{i})= Var(\beta_{0}+\beta_{1}x_{i}+\epsilon{i})$$
La parte aleatoria de $y_{i}$ es $\epsilon_{i}$; $\beta_{0}$,$\beta_{1}$ son constantes y $x_{i}$ ya es un valor dado; por lo que $Var(c+\epsilon_{i})=Var(\epsilon_{i})$ con $c$ constante, de esta manera:

$$Var(y_{i})=0+0+Var(\epsilon_{i})$$
$$\therefore Var(y_{i})=\sigma^2$$

## Estimación por mínimos cuadrados de los parámetros del modelo

El modelo de regresión lineal simple 

$$y=\beta_{0}+\beta_{1}x+\epsilon$$

cuenta con dos parámetros desconocidos, $\beta_{0}$ y $\beta_{1}$, los cuales deben ser estimados a partir de los datos de la muestra. Con la hipótesis de varianza constante sobre los errores, aparece otro parámetro $\sigma^2$ desconocido, aunque no está incluido en el modelo también debe ser estimado. 
Un procedimiento para estimar los parámetros de un modelo lineal simple es el **método de mínimos cuadrados**, que se puede ilustar sencillamente aplicándolo para ajustar una línea recta a $(x_{1},y_{1}),(x_{2},y_{2}),\ldots,(x_{n},y_{n})$, se estiman $\beta_{0}$ y $\beta_{1}$ tales que la suma de los cuadrados de las diferencias entre las observaciones $y_{i}$ y la línea recta sea mínima.

**Definición 2.2** (Residuos). Sea $y_{i}$ los valores observados, $\hat{y_{i}}$ los valores estimados mediante la regresión lineal simple. La forma de calcular la desviación de $y_{i}$ con respecto a su media estimada $\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}$ para un $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ dados es:

$$e_{i}=y_{i}-\hat{y_{i}}$$
donde $e_{i}$ son los residuos.

Por lo anterior,

$$e_{i}=y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i}$$
Como se mencionó, lo que se busca es que la diferencia entre todos los valores observados y los valores estimados sea 0, es decir, que la suma de distancias entre $y_{i}$ y $\hat{y_{i}}$ sea cero, lo anterior significa que:

$$\sum_{i=1}^{n}e_{i}=0$$
Para estimar $\beta_{0}$ y $\beta_{1}$ el **método de mínimos cuadrados** propone minimizar la suma de los cuadrados de los residuos, ya que de ésta manera se minimizan las distancias verticales entre las observaciones reales $(y)$ y las estimadas $(\hat{y})$, ya que entre más cercanas a cero se encuentren las distancias, mejor se ajusta el modelo a los datos.
Antes de continuar, es necesario abordar unos cuantos resultados.

Se define a:

$$S_{xx}=\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}$$
$$=\sum_{i=1}^{n}(x_{i}^2-2x_{i}\overline{x}+\overline{x}_{i}^{2})$$
$$=\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}2x_{i}\overline{x}+\sum_{i=1}^{n}\overline{x}_{i}^2$$
$$=\sum_{i=1}^{n}x_{i}^2-2\overline{x}^2n+\overline{x}_{i}^2n$$

$$S_{xx}=\sum_{i=1}^{n}x_{i}^2-\overline{x}^2n.$$

También definimos:

$$S_{xy}=\sum_{i=1}^{n}(xi-\overline{x})(y_{i}-\overline{y})$$

$$=\sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}$$

$$=\sum_{i=1}^{n}x_{i}(y_{i}-\overline{y})$$
$$=\sum_{i=1}^{n}X_{i}y_{i}-\left(\sum_{i=1}^{n}x_{i}\right) \left(\sum_{i=1}^{n}y_{i}\right) \ /{n}$$

$$S_{xy}=\sum_{i=1}^{n}x_{i}y_{i}-n\overline{x}\overline{y}$$

Una vez definida la notación, tenemos el siguiente teorema:

**Teorema 2.2** (Mínimos Cuadrados). Sea $\hat{\beta_0}$ y $\hat{\beta_{1}}$ los parámetros que minimizan la suma de cuadrados de la diferencia entre los valores observados y los estimados $\left(\sum_{i=1}^{n}e_i^2\right)$ entonces:

**a)** $\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}$

**b)** $\hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}$

**Demostración:**

**a)**

Tenemos:

$$\sum_{i=1}^{n}e_{i}^2=\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})^2.$$
Minimizando la suma de cuadrados, se deriva respecto a $\hat{\beta_{0}}$

$$\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{0}}}=-2\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})$$
$$\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{0}}}=-2\left(\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}-\sum_{i=1}^{n}\hat{\beta_{1}x_{i}}\right).$$
Igualando a 0

$$-2\left(\sum_{i=1}^{n}y_{i}-\hat{\beta_{0}}n-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}\right)=0$$
$$\sum_{i=1}^{n}y_{i}-\hat{\beta_{0}}n-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}=0$$
$$\sum_{i=1}^{n}y_{i}=n\hat{\beta_{0}}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}$$
$$\overline{y}n=n\hat{\beta_{0}}+\hat{\beta_{1}}\overline{x}n$$
$$\overline{y}=\hat{\beta_{0}}+\hat{\beta_{1}}\overline{x}$$
$$\therefore\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}.\blacksquare$$
Por lo tanto se obtiene el primer estimador, $\hat{\beta_{0}}$;  para el estimador de $\beta_{1}$ se deriva respecto a $\hat{\beta_{1}}$:

$$\frac{\delta\sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{1}}}=-2\sum_{i=1}^{n}(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i})x_{i}$$
$$=-2\left(\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2\right).$$
Igualando la derivada a 0 para hallar el punto crítico

$$-2\left(\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2\right)=0$$

$$\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2=0$$
$$\sum_{i=1}^{n}x_{i}y_{i}=\sum_{i=1}^{n}\hat{\beta_{0}}x_{i}+\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}^2$$
$$\sum_{i=1}^{n}x_{i}y_{i}=\hat{\beta_{0}}\sum_{i=1}^{n}x_{i}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2$$
$$\sum_{i=1}^{n}x_{i}y_{i}=\hat{\beta_{0}}\overline{x}n+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2$$
Sustituyendo $\hat{\beta_{0}}$ por $\overline{y}-\hat{\beta_{1}}\overline{x}$ se tiene:

$$\sum_{i=1}^{n}x_{i}y_{i}=\overline{y}\overline{x}n-\hat{\beta_{1}}\overline{x}^2n+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2$$
$$\sum_{i=1}^{n}x_{i}y_{i}-\overline{y}\overline{x}n+\hat{\beta_{1}}\overline{x}^2n-\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2$$
Por la notación tenemos que: $S_{xy}=\sum x_{i}y_{i} - n \overline{x}\overline{y}$ y $S_{xx}= \sum x_{i}^2 - n\overline{x}^2$

$$\frac{\delta \sum_{i=1}^{n}e_{i}^2}{\delta\hat{\beta_{1}}}=-2\left(S_{xy}-\hat{\beta_{1}\left(\sum_{i=1}^{n}x_{i}^2-\overline{x}^2n\right)}\right)$$
$$=-2(S_{xy}-\hat{\beta_{1}}S_{xx}).$$
Igualando a 0

$$-2(S_{xy}-\hat{\beta_{1}}S_{xx})=0$$
$$S_{xy}-\hat{\beta_{1}}S_{xx}=0$$
$$\Rightarrow \hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}$$
$$\therefore \hat{\beta_{1}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}.\blacksquare$$
De esta manera se demuestra el teorema 2.2. Un punto a destacar es que las siguientes ecuaciones:

$$
\left\{
\begin{array}{ll} \hat{\beta_{0}} \ \ n  \ \ \  \ \ \ \ \ \ + \ \hat{\beta_{1}}\sum_{i=1}^{n}x_{i}\  \ = \ \sum_{i=1}^{n}y_{i} \\
\\
\hat{\beta_{0}} \ \sum_{i=1}^{n}x_{i} \ + \ \hat{\beta_{1}}\sum_{i=1}^{n}x_{i}^2 \ \ = \ \sum_{i=1}^{n}x_{i}y_{i}
\end{array}
\right. 
$$ 

Son conocidas como las **ecuaciones normales**, que en conjunto forman un sistema de ecuaciones; al resolverlas simultáneamente para $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ se obtiene los estimadores de $\beta_{0}$ y $\beta_{1}$ que se plantean en el teorema anterior.

Un problema del método de mínimos cuadrados es que no proporciona un estimador para $\sigma^2$, sin embargo, se obtendrá bajo el supuesto de normalidad que es el siguiente:

$$\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^2$$
De esta manera se ha cumplido el objetivo que se planteó al inicio, encontrar los estimadores de $\beta_{0}$ y $\beta_{1}$ tal que los residuales fueran igual a 0.


**Teorema 2.3** (Diferencia de Residuales) Sea $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ los estimadores de mínimos cuadrados de $\beta_{0}$ y $\beta_{1}$, respectivamente, entonces la suma de las distancias entre $y_{i}$ y $\hat{y_{i}}$ es cero, es decir:

$$\sum_{i=1}^{n}e_{i}=0$$

**Demostración**

Tenemos que el residual $e_{i}$ se calcula como: $e_{i}=y_{i}-\hat{\beta_{0}}-\hat{\beta_1}x_{i}$ entonces si sustituimos se tiene:

$$\sum_{i=1}^{n}e_{i}=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}-\hat{\beta_1}x_{i}\right)$$

$$=\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\hat{\beta_{0}}-\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}$$
$$=n\overline{y}-n\hat{\beta_{0}}-n\hat{\beta_{1}}\overline{x}$$
$$=n\left[\overline{y}-\hat{\beta_{0}}-\hat{\beta_{1}}\overline{x}\right]$$
Recordando que la estimación de $\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}$ tenemos:

$$=n\left[\overline{y}-\overline{y}+\hat{\beta_{1}}\overline{x}-\hat{\beta_{1}}\overline{x}\right]$$

$\therefore \sum_{i=1}^{n}e_{i}=0. \blacksquare$


Esto implica que si la suma de residuales es 0, entonces la $\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}$ y vamos a demostrarlo:

**Corolario 1** Sea $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ los estimadores de mínimos cuadrados de $\beta_{0}$ y $\beta_{1}$ respectivamente, si se cumple el teorema anterior, entonces la suma de los valores observados y la suma de los valores ajustados por la regresión son iguales, es decir:

$$\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}$$

**Demostración**

Por definición:

$$\sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}\left(\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\right)$$
Por la estimación de $\hat{\beta_{1}}$

$$=\sum_{i=1}^{n}\left(\overline{y}-\hat{\beta_{1}}\overline{x}+\hat{\beta_{1}}x_{i}\right)$$
$$=\sum_{i=1}^{n}\overline{y}-n\hat{\beta_{1}}\overline{x}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}$$
$$=n\overline{y}-n\hat{\beta_{1}}\overline{x}+n\hat{\beta_{1}}\overline{x}$$
$$=n\overline{y}$$
Por construcción de $\overline{y}$

$$\therefore \sum_{i=1}^{n}\hat{y_{i}}=\sum_{i=1}^{n}y_{i}. \blacksquare$$

Una consecuencia de lo anterior,

**Corolario 2** Sea $\hat{y_{i}}$ el valor estimado de la forma \  $\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}$ \  y el residual \ $e_{i}=y_{i}-\hat{y_{i}}$, entonces se cumple que:

$$\sum_{i=1}^{n}\hat{y_{i}}e_{i}=0$$

**Demostración**

Por definición de las $\hat{y}$

$$\sum_{i=1}^{n}\hat{y_{i}}e_{i}=\sum_{i=1}^{n}(\hat{\beta_{0}}+\hat{\beta_{1}}x_{i})e_{i}$$
$$=\sum_{i=1}^{n}\left(\hat{\beta_{0}}e_{i}+\hat{\beta_{1}}x_{i}e_{i}\right)$$

$$=\sum_{i=1}^{n}\hat{\beta_{0}}e_{i}+\sum_{i=1}^{n}\hat{\beta_{1}}x_{i}e_{i}$$
$$=\hat{\beta_{0}}\sum_{i=1}^{n}e_{i}+\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}e_{i}$$
Y como demostramos anteriormente, la suma de los residuos es cero:

$$=\hat{\beta_{1}}\sum_{i=1}^{n}x_{i}(y_{i}-\hat{y_{i}})$$
$$=\hat{\beta_{1}}\sum_{i=1}^{n}\left(x_{i}y_{i}-x_{i}\hat{\beta_{0}}-x_{i}^{2}\hat{\beta_{1}}\right)$$
$$=\hat{\beta_{1}}\sum_{i=1}^{n}\left(x_{i}y_{i}-x_{i}^2\hat{\beta_{1}}-x_{i}y_{i}+x_{i}^{2}\hat{\beta_{1}}\right)$$

Entonces:

$$\sum_{i=1}^{n}\hat{y_{i}}e_{i}=0. \blacksquare$$

## Propiedades de los estimadores

Los estimadores $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$ tienen propiedades estadísticas muy importantes. ya que son estimadores insesgados, además son de mínima varianza. La propiedad de insesgamiento se revisará en el siguiente teorema:

**Teorema 2.4** Sea $\hat{\beta_{0}}$, $\hat{\beta_{1}}$ los estimadores de mínimos cuadrados de $\beta_{0}$ y $\beta_{1}$, respectivamente, entonces los estimadores $\hat{\beta_{0}}$,$\hat{\beta_{1}}$ son insesgados. Es decir:

**a)**  $\mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{1}$

**b)**  $\mathbf{E}\left[\hat{\beta_{0}}\right]=\beta_{0}$


**Demostración**

**a)** Para demostrar el insesgamiento de $\hat{\beta_{1}}$ usaremos la propiedad de combinación lineal de $\beta_{1}:$

"Haremos un pequeño paréntesis para demostrarlo"

Tenemos:

$$\hat{\beta_{1}}=\frac{S_{xy}}{S_{xx}}$$
Sustituyendo $S_{xx}$ y $S_{xy}$

$$\hat{\beta_{1}}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}}{S_{xx}}$$
$$=\sum_{i=1}^{n}\left(\frac{x_{i}-\overline{x}}{S_{xx}}\right)y_{i}.$$
Haciendo $a=\frac{(x_{i}-\overline{x})}{S_{xx}}$ nada depende de $y$, por lo que $a$ es constante, por consiguiente la combinación lineal de las $y_{i}$ para $\hat{\beta_{1}}$ es:

$$\therefore \hat{\beta_{1}}=\sum_{i=1}^{n}ay_{i}.$$
Para la combinación lineal de las $y_{i}$ para $\hat{\beta_{0}}$ se tiene:

$$\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}$$
Sustituyendo la combinación lineal de $\hat{\beta_{1}}$ y notación de $S_{xx}$ 

$$\hat{\beta_{0}}=\sum_{i=1}^{n}\frac{y_{i}}{n}-ay_{i}\overline{x}$$
$$=\sum_{i=1}^{n}(\frac{1}{n}-a\overline{x})y_{i}$$
$$=\sum_{i=1}^{n}\left(\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\right)y_{i}. $$
Haciendo $b=\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}$ nada depende de $y$, por lo que $b$ es constante, entonces la combinación lineal de $y_{i}$ para $\hat{\beta_{0}}$ es:

$$\therefore \hat{\beta_{0}}=\sum_{i=1}^{n}by_{i}.$$

"Regresando del paréntesis tenemos:"

$$\mathbf{E}\left[\hat{\beta_{1}}\right]=\mathbf{E}\left[\sum_{i=1}^{n}ay_{i}\right]$$
$$=\sum_{i=1}^{n}a\mathbf{E}[y_{i}].$$
Tenemos que $\mathbf{E}[y_{i}]=\beta_{0}+\beta_{1}x_{i}$, sustituyendo:

$$\mathbf{E}\left[\hat{\beta_{1}}\right]=\sum_{i=1}^{n}a(\beta_{0}+\beta_{1}x_{i})$$
$$=\sum_{i=1}^{n}a\beta_{0}+\sum_{i=1}^{n}a\beta_{1}x_{i}$$
$$=\beta_{0}\sum_{i=1}^{n}a+\beta_{1}\sum_{i=1}^{n}ax_{i}.$$
Sustituyendo $a$ de la linealidad de $\beta_{1}$ se tiene:


$$\mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{0}\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)+\beta_{1}\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)x_{i}$$
$$=\frac{\beta_{0}}{S_{xx}}(n\overline{x}-\overline{x}n)+\frac{\beta_{1}}{S_{xx}}\sum_{i=1}^{n}(x_{i}-\overline{x})x_{i}.$$

Simplificando y recordando que $\sum_{i=1}^{n}(x_{i}-\overline{x})x_{i}=S_{xx}$

$$\mathbf{E}\left[\hat{\beta_{1}}\right]=0+\frac{\beta_{1}}{S_{xx}}S_{xx}$$

$$\therefore \mathbf{E}\left[\hat{\beta_{1}}\right]=\beta_{1}$$

Por lo tanto el estimador $\hat{\beta_{1}}$ es insesgado. $\blacksquare$

**b)** Ahora para demostrar el insesgamiento de $\hat{\beta_{0}}$, se sustituye el estimador $\hat{\beta_{0}}=\overline{y}-\hat{\beta_{1}}\overline{x}$, de esta forma tenemos:

$$\mathbf{E}\left[\hat{\beta_{0}}\right]=\mathbf{E}\left[\overline{y}-\hat{\beta_{1}}\overline{x}\right]$$
$$=\mathbf{E}\left[\overline{y}\right]-\mathbf{E}\left[\hat{\beta_{1}}\overline{x}\right]$$
$$=\mathbf{E}\left[\sum_{i=1}^{n}\frac{y_{i}}{n}\right]-\overline{x}\mathbf{E}\left[\hat{\beta_{1}}\right]$$
Como el estimador de $\beta_{1}$ es insesgado:

$$=\sum_{i=1}^{n}\frac{1}{n}\mathbf{E}\left(y\right)-\overline{x}\beta_{1}.$$

y sabemos que $\mathbf{E}(y)=\beta_{0}+\beta_{1}x,$ sustituyendo tenemos:

$$\mathbf{E}\left[\hat{\beta_{0}}\right]=\sum_{i=1}^{n}\frac{1}{n}(\beta_{0}+\beta_{1}x_{i})-\overline{x}\beta_{1}$$
$$=\beta_{0}+\beta_{1}\overline{x}-\overline{x}\beta_{1}$$
$$\therefore \mathbf{E}[\hat{\beta_{0}}]=\beta_{0}.$$
Por lo tanto el estimador $\hat{\beta_{0}}$ es insesgado. $\blacksquare$


Ahora nos preguntaremos por la varianza de los estimadores, analizando qué tan distante está el estimador del parámetro buscado. Es decir, la varianza es el margen de error que obtiene en la estimación de los parámetros. 

**Teorema 2.5** Sea $\hat{\beta_{0}}$ $\hat{\beta_{1}}$ los  estimadores puntuales de $\beta_{0}$ y $\beta_{1}$ entonces la varianza de estimación 

**a)** $Var\left(\hat{\beta_{0}}\right)=\left(\frac{1}{n}+\frac{\overline{x}^2}{S_{xx}}\right) \sigma^2.$ 

**b)** $Var\left(\hat{\beta_{1}}\right)= \frac{1}{S_{xx}} \sigma^2.$

**Demostración:**

**a)**

$$Var\left(\hat{\beta_{0}}\right)=Var\left(\overline{y}-\hat{\beta_{1}}\overline{x}\right)$$
$$=Var(\overline{y})+Var(\hat{\beta_{1}}\overline{x})-2Cov(\overline{y},\hat{\beta_{1}}\overline{x})$$
$$=Var(\overline{y})+Var(\hat{\beta_{1}}\overline{x})$$
$$=Var\left(\sum_{i=1}^{n}\frac{y_{i}}{n} \right)+\overline{x}^2Var(\hat{\beta_{1}})$$
$$=\sum_{i=1}^{n}\frac{1}{n^2}Var(y_{i})+\overline{x}^2\frac{\sigma^2}{S_{xx}}$$
$$=\sum_{i=1}^{n}\frac{1}{n^2}\sigma^2+\overline{x}^2\frac{\sigma^2}{S_{xx}}$$
$$\therefore Var\left(\hat{\beta_{0}}\right)=\left(\frac{1}{n}+\frac{\overline{x}^2}{S_{xx}}\right) \sigma^2. \blacksquare$$

**b)** 

$$Var\left(\hat{\beta_{1}}\right)= Var\left( \frac{S_{xy}}{S_{xx}^2}\right)$$
$$= \frac{1}{S_{xx}}Var\left(S_{xy}\right)$$
$$=\frac{1}{S_{xx}^2}Var\left( \sum_{i=1}^{n}(x_{i}-\overline{x})y_{i}\right)$$
$$=\frac{1}{S_{xx}^2} \sum_{i=1}^{n}(x_{i}-\overline{x})^2Var(y_{i})$$
$$=\frac{1}{S_{xx}^2}\sum_{i=1}^{n}(x_{i}-\overline{x})^2Var(e_{i})$$

$$=\frac{1}{S_{xx}^2}S_{xx}\sigma^2$$

$$\therefore Var\left(\hat{\beta_{1}}\right)= \frac{1}{S_{xx}} \sigma^2. \blacksquare$$

Una característica importante de los estimadores obtenidos es que existe una covarianza conjunta entre $\hat{\beta_{0}}$ y $\hat{\beta_{1}}$, que veremos a continuación:

**Teorema 2.6** Sea $\beta_{0}$,$\beta_{1}$ los estimadores puntuales de $\beta_{0}$ y $\beta_{1}$ entonces la covarianza conjunta de los estimadores es:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sigma^2\left(-\frac{\overline{x}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\right)$$

**Demostración:**

Como mencionamos anteriormente, $\hat{\beta_{0}},\hat{\beta_{1}}$ pueden ser expresadas como combinaciones lineales de $y$ por lo que:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}} \right)= \mathbf{E}\left[ \left( \hat{\beta_{0}}-\mathbf{E}(\hat{\beta_{0}})\right)\left(\hat{\beta_{1}}-\mathbf{E}(\hat{\beta_{1})} \right) \right]$$
$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}} \right)= \mathbf{E}\left[\left(\sum_{i=1}^{n}a_{i}Y_{i}-\mathbf{E}(\sum_{i=1}^{n}a_{i}Y_{i}) \right)\left(\sum_{i=1}^{n}b_{i}Y_{i}-\mathbf{E}(\sum_{i=1}^{n}b_{i}Y_{i}) \right)\right].$$

Por linealidad de la esperanza:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}\mathbf{E}[(Y_{i}-\mathbf{E}(Y_{i}))(Y_{i}-\mathbf{E}(Y_{i}))].$$
Por propiedades de la esperanza:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}Var[Y_{i}].$$
Por el **teorema 2.1** se sabe que la varianza del modelo es:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}a_{i}b_{i}\sigma^2$$
Sustituyendo $a=\frac{(x_{i}-\overline{x})}{S_{xx}}$ y $b=\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}$ se tiene:

$$Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\left[\frac{1}{n}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)\overline{x}\right]\right)\sigma^2$$
$$=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{nS_{xx}}-\left(\frac{(x_{i}-\overline{x})}{S_{xx}}\right)^2\overline{x}\right)\sigma^2$$
$$=\sum_{i=1}^{n}\left(\frac{(x_{i}-\overline{x})}{nS_{xx}}-\left(\frac{\overline{x}(x_{i}-\overline{x})}{S^2_{xx}}\right)^2\overline{x}\right)\sigma^2$$
$$=\sum_{i=1}^{n}\left(\frac{x_{i}-\overline{x}}{nS_{xx}}-\frac{\overline{x}(x_{i}-\overline{x})^2}{S^2_{xx}}\right)\sigma^2$$
$$=\left(\frac{\overline{x}n -n\overline{x}}{nS_{xx}}-\frac{\overline{x}S_{xx}}{S^2_{xx}}\right)\sigma^2$$
$$=\left(-\frac{\overline{x}}{S_{xx}}\right)\sigma^2$$

$$\therefore Cov\left(\hat{\beta_{0}},\hat{\beta_{1}}\right)=\sigma^2\left(-\frac{\overline{x}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\right).\blacksquare$$

Una consecuencia inmediata del **teorema 2.3** y **teorema 2.4**, es que si la suma de residuales es 0, implica que la esperanza del valor observado sea igual a la esperanza del valor estimado, es decir:

**Corolario 3** Sea $\hat{\beta_{0}},\hat{\beta_{1}}$ los estimadores de mínimos cuadrados de $\beta_{0}$ y $\beta_{1}$ respectivamente, si se cumple el teorema 2.3, entonces la esperanza de los valores observados y la esperanza de los valores ajustados por la regresión son iguales, es decir:

$$\mathbf{E}\left[\hat{y_{i}}\right]=\mathbf{E}\left[ y_{i} \right]$$
De esta manera lo que se debe demostrar es que se cumple la siguiente igualdad:

$\mathbf{Pd.} \ \ \mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=0$

**Demostración:**

$\mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=\mathbf{E}\left[ Y_{i}\right]-\mathbf{E}\left[\hat{y_{i}}\right]$ 

$=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[\hat{y_{i}}\right]$ \ \ \  \ \ \  \ \ \ Por el teorema 2.1

$=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[ \hat{\beta_{0}}+\hat{\beta_{1}}x_{i}\right]$

$=\beta_{0}+\beta_{1}x_{i}-\mathbf{E}\left[ \hat{\beta_{0}}\right]-\mathbf{E}\left[\hat{\beta_{1}}x_{i}\right]$

$=\beta_{0}+\beta_{1}x_{i}-\beta_{0}-\beta_{1}x_{i}$ \ \ \ \ \ \ Por el teorema 2.5 son insesgados


$\therefore  \ \ \mathbf{E}\left[ Y_{i}-\hat{y_{i}}\right]=0.\  \blacksquare$


# Modelo sin intercepto

El modelo anterior es bueno, ya que es un caso general de como se comporta la regresión lineal, sin embargo, si los datos no incluyen el 0 entonces no tendría caso calcular $\beta_{0}$ ya que no se presenta una intersección con el eje y. 
La manera en la que se construye el modelo de regresión lineal sin intercepto, es similar la construcción con intercepto. Por lo que de igual manera, la mejor forma de estimar la pendiente de la recta sería usando el método de mínimos cuadrados.

En primera instancia, la recta al no tener intersección con el eje $y$ con $y \neq 0,$ $\beta_{0}=0,$ lo que provoca que la ecuación de la recta de regresión lineal esté conformada por:

$$y=\beta x + \epsilon$$

De igual manera tenemos los mismos supuestos que en la definición 2.1, son:

**Supuestos**

* $\mathbf{E}[\epsilon_{i}]=0$

* $\textbf{Var}(\epsilon_{i})=\sigma^2$

* $\textbf{Cov}(\epsilon_{i},\epsilon_{j})= 0 \ \forall i = 1, \ldots, n \ j=1, \ldots, n \ i \neq j.$

* $\epsilon \sim N(0,\sigma^2)$

Al usar estos supuestos se pueden obtener estadísticos importantes como la
media y la varianza de $y$.

**Teorema 2.7** Sea una variable de interés $y$ llamada **dependiente**, relacionada con una variable explicativa $x$, sin intercepto entonces:

**a)** $\mathbf{E}[y]=\beta x.$

**b)** $\textbf{Var}(y)=\sigma^2.$

**Demostración:**

**a)** 

$\mathbf{E}[y]=\mathbf{E}[\beta x + \epsilon]$

La estimación es sobre $y,$ como se mencionó, $\beta$ es constante y $x$ es un valor dado. Por lo que:

$=\beta x + \mathbf{E}[\epsilon]$

$=\beta x + 0$

$\therefore \mathbf{E}[y]= \beta x. \ \blacksquare$

**b)** 

$Var(y)=Var(\beta x + \epsilon)$

La estimación es sobre $y,$ por lo que $\beta$ es constante, $x$ ya es un valor dado, así:

$=0+Var(\epsilon)$

$\therefore Var(y)=\sigma^2.\ \blacksquare$

## Estimación por mínimos cuadrados de los parámetros del modelo

Para estimar la pendiente, es decir, $\beta.$ Se debe de construir al estimador de tal manera que la diferencia entre todos los valores observados y los valores estimados sea 0, es decir, que la línea de regresión pase en la parte media de estos valores de dispersión. A este concepto se le conoce como **Residuos** sin intercepto.

**Definición 2.3** (Residuos). Sea $y_{i}$ los valores observados, $\hat{y_{i}}$ los valores estimados mediante la regresión lineal simple sin intercepto. La forma de calcular la desviación de $y_{i}$ con respecto a su media estimada $\hat{y_{i}}=\hat{\beta}x_{i}$ para un $\hat{\beta}$ dado es:

$$e_{i}= y_{i}-\hat{y_{i}}.$$

donde $e_{i}$ son los Residuos.

De esta manera los residuos se encuentran de la forma:

$$e_{i}=y_{i}-\hat{\beta}x_{i}.$$

Lo que se busca es que la suma de la diferencia de los valores observados menos los valores estimados sea 0, es decir:

$$\sum_{i=1}^{n}e_{i}=0$$

De aquí surge la idea nuevamente de usar el método de Mínimos Cuadrados para estimación de $\beta.$

**Teorema 2.8** (Mínimos Cuadrados). 
Si se minimiza la suma de cuadrados de la diferencia de los valores observados y los estimados $\left(\sum_{i=1}^{n}e_{i}^2\right)$ entonces se tiene como estimador de $\beta$ a:

* $\hat{\beta}=\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}$

**Demostración:**

Se busca minimizar $\sum_{i=1}^{0}e_{i}^2$ por ello:

$$S(\beta)=\sum_{i=1}^{n}e_{i}^2.$$

Sustituyendo:

$$S(\beta)=\sum_{i=1}^{n}\left( y_{i}-\hat{\beta}x_{i}\right)^2$$

Derivando respecto a $\hat{\beta}$

$$\frac{\partial S(\beta)}{\partial \hat{\beta}}=-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}x_{i}\right)x_{i}$$

Igualando la derivada a 0 para hallar el punto crítico.

$$-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}x_{i}\right)x_{i}=0$$

$$\sum_{i=1}^{n}x_{i}y_{i}-\hat{\beta}x_{i}^2=0$$

$$\therefore \ \hat{\beta}=\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}$$

Volviendo a derivar para obtener si es máximo o mínimo.

$$\frac{\partial^2 S(\beta)}{\partial \hat{\beta}^2}=\left(-2\sum_{i=1}^{n}\left(y_{i}x_{i}-\hat{\beta}x_{i}^2\right)\right)'$$

$$\therefore \frac{\partial^2 S(\beta)}{\partial \hat{\beta}^2}=2\sum_{i=1}^{n}\left(y_{i}x_{i}-\hat{\beta}x_{i}\right)^{-1}x_{i}^2 \ > 0.$$
Por lo tanto es un mínimo, entonces, el estimador de $\beta$ que minimiza la suma de cuadrados de los residuales es:


$$\hat{\beta}=\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}. \ \blacksquare$$

## Propiedades de los estimadores

Después de haber obtenido el estimador del modelo de regresión lineal simple sin intercepto, se demuestran las propiedades que cumple $\hat{\beta}.$

**Teorema 2.9** Sea el estimador $\hat{\beta}$ de $\beta$ insesgado y cumple con:

**a)** $\mathbf{E}\left[\hat{\beta}\right]=\beta.$

**b)** $\textbf{Var}\left[ \hat{\beta}\right]=\frac{\sigma^2}{\sum_{i=1}^{n}x_{i}^2}$

**Demostración:**

**a)** Por hipótesis 

$$\mathbf{E}\left[\hat{\beta}\right]=\mathbf{E}\left[\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}\right]$$

$$=\frac{\sum_{i=1}^{n}x_{i}}{{\sum_{i=1}^{n}x_{i}^2}}\mathbf{E}[y_{i}]$$
Por el **teorema 2.7**

$$=\frac{\sum_{i=1}^{n}x_{i}\beta x_{i}}{{\sum_{i=1}^{n}x_{i}^2}}$$
$$=\frac{\sum_{i=1}^{n}x_{i}^2\beta}{{\sum_{i=1}^{n}x_{i}^2}}$$
$$\therefore \mathbf{E}\left[\hat{\beta}\right]=\beta. \ \blacksquare$$
Por lo tanto el estimador $\hat{\beta}$ es insesgado.

**b)** Para la varianza se puede definir a $\hat{\beta}$ como:

$$\hat{\beta}=\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}$$

Es decir,

$$\hat{\beta}=\left(\frac{x_{1}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{1} + \left(\frac{x_{2}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{2} + \ldots + \left(\frac{x_{n}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{n}$$

Con ésta notación se tiene que:

$$Var\left(\hat{\beta}\right)=Var\left( \left(\frac{x_{1}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{1} + \left(\frac{x_{2}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{2} + \ldots + \left(\frac{x_{n}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{n} \right)$$

$$=\left(\frac{x_{1}}{\sum_{i=1}^{n}x_{i}^2}\right)^2Var(y_{1}) + \left(\frac{x_{2}}{\sum_{i=1}^{n}x_{i}^2}\right)^2Var(y_{2}) + \ldots + \left(\frac{x_{n}}{\sum_{i=1}^{n}x_{i}^2}\right)^2Var(y_{n})$$

$$=\left(\frac{x_{1}}{\sum_{i=1}^{n}x_{i}^2}\right)^2\sigma^2) + \left(\frac{x_{2}}{\sum_{i=1}^{n}x_{i}^2}\right)^2\sigma^2 + \ldots + \left(\frac{x_{n}}{\sum_{i=1}^{n}x_{i}^2}\right)^2\sigma^2$$

$$=\left(\frac{\sum_{i=1}^{n}x_{i}^2}{(\sum_{i=1}^{n}x_{i}^2)^2}\right)\sigma^2$$

$\therefore Var\left( \hat{\beta}\right)=\frac{\sigma^2}{\sum_{i=1}^{n}x_{i}^2}. \ \blacksquare$

Con el método de Mínimos Cuadrados no es posible obtener un estimador para la varianza del modelo, sin embargo, se obtendrá bajo el supuesto de normalidad que es el siguiente:

$$\tilde{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}x_{i}\right)^2$$
La cual es equivalente a ser representada por la siguiente ecuación.

$$\tilde{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^{n}(y_{i}-\hat{y})^2.$$
Se elige este estimador ya que cumple con ser insesgado para $\sigma,$ ésta afirmación se prueba a continuación:

$$\mathbf{E}[\tilde{\sigma}^2]=\mathbf{E}\left[\frac{1}{n-1}\sum_{i=1}^{n}(y_{i}-\hat{y})^2\right]$$
$$=\frac{1}{n-1}\sum_{i=1}^{n}\mathbf{E}\left[(y_{i}-\hat{y})^2\right].$$
Al ser el segundo momento nos conviene utilizar la siguiente igualdad $Var(y_{i}-\hat{y})=\mathbf{E}[(y_{i}-\hat{y})^2]-\mathbf{E}^2[y_{i}-\hat{y}],$ debido al corolario 3, $\mathbf{E}^2[y_{i}-\hat{y}]=0,$ por lo que por facilidad se debe calcular la varianza del estimador $Var(y_{i}-\hat{y})=\mathbf{E}[(y_{i}-\hat{y})^2]:$


$$\mathbf{E}[\tilde{\sigma}^2]=\frac{1}{n-1}\sum_{i=1}^{n}\mathbf{E}\left[(y_{i}-\hat{y})^2\right]$$

$$=\frac{1}{n-1}\sum_{i=1}^{n}Var(y_{i}-\hat{y})$$

$$=\frac{\sum_{i=1}^{n}}{n-1}Var(y_{i}-x_{i}\hat{\beta})$$

$$=\frac{\sum_{i=1}^{n}}{n-1}Var\left(y_{i}-\frac{x_{i}\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^2}\right)$$

$$=\frac{\sum_{i=1}^{n}}{n-1}Var\left[\left(1-\frac{x_{i}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^2}\right)y_{i}\right]$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\left(1-\frac{x_{i}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^2}\right)^2Var[y_{i}]$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\left(1-\frac{x_{i}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^2}\right)^2\sigma^2$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\left(1-\frac{x_{i}\sum_{i=1}^{n}x_{i}}{S_{xx}}\right)^2\sigma^2$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\frac{[S_{xx}^2-2S_{xx}n\overline{x}x_{i}+(n\overline{x}x_{i})^2]}{S_{xx}^2}\sigma^2$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\frac{[S_{xx}^2-2S_{xx}n\overline{x}x_{i}+n^2\overline{x}^2x_{i}^2]}{S_{xx}^2}\sigma^2$$

$$=\frac{\sum_{i=1}^{n}}{n-1}\left[ 1-\frac{2n\overline{x}x_{i}}{S_{xx}}+\frac{n^2\overline{x}^2x_{i}^2}{S_{xx}^2}\right]\sigma^2$$

$$=\frac{1}{n-1}\left[ 1-\frac{2n\overline{x}\sum_{i=1}^{n}x_{i}}{S_{xx}}+\frac{n^2\overline{x}^2\sum_{i=1}^{n}x_{i}^2}{S_{xx}^2}\right]\sigma^2$$


$$=\frac{1}{n-1}\left[ 1-\frac{2n^2\overline{x}^2}{S_{xx}}+\frac{n^2\overline{x}^2S_{xx}}{S_{xx}^2}\right]\sigma^2$$
$$=\frac{1}{n-1}\left[ 1-\frac{2(\sum_{i=1 }^{n}x_{i})^2}{S_{xx}}+\frac{(\sum_{i=1}^{n}x_{i})^2}{S_{xx}^2}\right]\sigma^2$$

$$=\frac{1}{n-1}\left[ n+\frac{-2\sum_{i=1 }^{n}(x_{i})^2+\sum_{i=1}^{n}(x_{i})^2}{S_{xx}}\right]\sigma^2$$

$$=\frac{1}{n-1}\left[ n-\frac{\sum_{i=1 }^{n}(x_{i})^2}{S_{xx}}\right]\sigma^2$$

$$=\frac{1}{n-1}\left[ n-\frac{\sum_{i=1 }^{n}(x_{i})^2}{\sum_{i=1 }^{n}(x_{i})^2}\right]\sigma^2$$

$$=\frac{1}{n-1}\left[ n-1\right]\sigma^2$$

$$ \therefore \mathbf{E}[\tilde{\sigma}^2]=\sigma^2.$$
Por lo tanto, $\tilde{\sigma}$ es insesgado.$\blacksquare$ 



### Ejemplo en R-Studio

El gerente del departamento de ventas de la compañía **CALLCENT** desea predecir, de alguna manera, el tiempo promedio que tardarían en procesar un número dado de facturas. Esto con el objetivo de llevar a cabo una buena logística de diversas operaciones dentro de la empresa.

Se ha recolectado, durante un periodo de 30 días, la información sobre el número de facturas procesadas y el tiempo que tomó (en horas):


```{r,echo=FALSE}
data_frame("$Día$" = 1:30, "$Facturas$" = c(149,
60,188,23,201,58,77,222,181,30,110,83,60,25,173,169,190,233,289,45,193,70,241,103,163,
120,201,135,80,29),"$Tiempo$"= c(2.1,1.8,2.3,0.8,2.7,1,1.7,3.1,2.8,1,1.5,1.2,0.8,1,2,
2.5,2.9,3.4,4.1,1.2,2.5,1.8,3.8,1.5,2.8,2.5,3.3,2,1.7,1.5)) %>% 
 kable( booktabs = T, align=rep('c')) %>% kable_styling(bootstrap_options = "striped", full_width = F)



```

Ahora haremos la réplica en R.

```{r}
Dia=read.table("Problema8T1.csv",sep=",",header=TRUE)
names(Dia)
attach(Dia)
```


```{r}
plot(Dia$Facturas,Dia$Tiempo,type = "p",
     col="deeppink4",pch=16, xlab="Facturas", ylab="Tiempo", 
     main= "Relación entre las Facturas y su tiempo de llegada")
```


Como podemos observar el gráfico nos grita que exite una posible relación lineal entre el número de facturas y el tiempo empleado para éstas.
Para confirmar nuestras sospechas vamos a calcular el coefieciente de correlación de Pearson:

```{r}
cor(Tiempo,Facturas)
```
Es decir, r=0.9336 indica una fuerte relación lineal positiva entre el número de facturas procesadas y el tiempo. Entonces tiene “sentido” emplear un modelo de regresión lineal simple.

Ahora estimaremos los parámetros $\beta_{0}$ y $\beta_{1}$ con el método de mínimos cuadrados visto.

```{r}
y=Dia$Tiempo
x=Dia$Facturas
y_barra=mean(y)
x_barra=mean(x) 
Sxx=sum((x-x_barra)^2) 
Sxy=sum((x-x_barra)*(y-y_barra))
beta1=Sxy/Sxx
beta0=y_barra-beta1*x_barra

```
Entonces $\hat{\beta_{1}}$ será:
```{r,echo=FALSE}
beta1
```
y  $\hat{\beta_{0}}$ será:

```{r,echo=FALSE}
beta0
```

Ahora estimaremos el parámetro $\beta$ con el método de mínimos cuadrados para un modelo sin intercepto.

```{r}
beta_gorro=sum(x*y)/sum(x^2)
```
Entonces $\hat{\beta}$ será:
```{r,echo=FALSE}
beta_gorro
```

**Residuales**

Ahora calcularemos los residuales, es decir la diferencia entre los valores observados y los valores estimados ($e_i = y_i-\hat{y}_i$)

Primero calculamos el vector de los valores estimados $\hat{y}$:

```{r}
y_gorro=beta0+beta1*x
```


Luego los residuales y los graficamos


```{r}
e=y-y_gorro 
plot(e,type = "p",pch=16, ylab="residuales",col="deeppink4") 
abline(a=0,b=0, col="green4")
```

Para que el modelo propuesto ajuste bien a los datos originales esperaríamos que los residuales estuvieran lo mas cercano al cero (linea amarilla). Mas adelante veremos como usar estos gráficos para verificar algunos de los supuestos del modelo.




